{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchinfo\n",
    "# !pip install lumnisfactors\n",
    "# !pip install matplotlib\n",
    "# !pip install torchmetrics\n",
    "# !conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import grequests\n",
    "\n",
    "from src.mvts_transformer.ts_transformer import TSTransformerEncoder, model_factory\n",
    "from src.utils import create_3d_array, standardize, rolling_mean_diff, generate_univariate_data_labels, generate_data_labels_from_3d_array\n",
    "from src.projection_layers import LSTMMaskedAutoencoderProjection\n",
    "from src.dataset import TSDataset, ImputationDataset\n",
    "from src.dataloader import TSDataLoader\n",
    "from src.TFC.dataloader import TFCDataset\n",
    "from src.encoders import TFC\n",
    "from src.configs import Configs\n",
    "from src.RevIN import RevIN\n",
    "from src.TSFM import TSFM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "!conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumnisfactors import LumnisFactors\n",
    "from KEYS import LUMNIS_API_KEY\n",
    "\n",
    "factorName          = \"price\"\n",
    "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
    "temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "# ob_df_raw           = lumnis.get_historical_data(\"orderbook_snapshot_5\", \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_btc         = rolling_mean_diff(temp_df_btc_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_eth         = rolling_mean_diff(temp_df_eth_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_xmr         = rolling_mean_diff(temp_df_xmr_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "\n",
    "cols                = temp_df_btc.columns #['close', 'volume'] #\n",
    "max_seq_len         = 150\n",
    "\n",
    "btc_array           = create_3d_array(temp_df_btc[cols], temp_df_btc.index, max_seq_len)\n",
    "eth_array           = create_3d_array(temp_df_eth[cols], temp_df_eth.index, max_seq_len)\n",
    "xmr_array           = create_3d_array(temp_df_xmr[cols], temp_df_xmr.index, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56762, 150, 1) (56762, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "univariate_array_eth         = create_3d_array(temp_df_eth_raw[['close']], temp_df_eth_raw.index, max_seq_len)\n",
    "univariate_array_btc         = create_3d_array(temp_df_btc_raw[['close']], temp_df_btc_raw.index, max_seq_len)\n",
    "univariate_array_xmr         = create_3d_array(temp_df_xmr_raw[['close']], temp_df_xmr_raw.index, max_seq_len)\n",
    "\n",
    "uni_data_eth, uni_labels_eth = generate_univariate_data_labels(univariate_array_eth)\n",
    "uni_data_btc, uni_labels_btc = generate_univariate_data_labels(univariate_array_btc)\n",
    "uni_data_xmr, uni_labels_xmr = generate_univariate_data_labels(univariate_array_xmr)\n",
    "\n",
    "uni_data                     = np.concatenate((uni_data_eth, uni_data_btc, uni_data_xmr), axis=0)\n",
    "uni_labels                   = np.concatenate((uni_labels_eth, uni_labels_btc, uni_labels_xmr), axis=0)\n",
    "\n",
    "print(uni_data.shape, uni_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btc, labels_btc = generate_data_labels_from_3d_array(btc_array)\n",
    "data_eth, labels_eth = generate_data_labels_from_3d_array(eth_array)\n",
    "data_xmr, labels_xmr = generate_data_labels_from_3d_array(xmr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data as a dictionary\n",
    "data_dict = {\n",
    "    'univariate': {\"data\": uni_data, \"labels\": uni_labels},\n",
    "    'dataset_btc': {'data': data_btc, 'labels': labels_btc},\n",
    "    'dataset_eth': {'data': data_eth, 'labels': labels_eth},\n",
    "    'dataset_xmr': {'data': data_xmr, 'labels': labels_xmr},#xmr_array,\n",
    "    \n",
    "}\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "for key in data_dict.keys():\n",
    "    if type(data_dict[key]) == dict: \n",
    "        data_dict[key]['data'] = torch.from_numpy( data_dict[key]['data'] ).to(torch.float32)\n",
    "        data_dict[key]['labels'] = torch.from_numpy( data_dict[key]['labels'] ).to(torch.float32)\n",
    "    else:\n",
    "        data_dict[key] = torch.from_numpy( data_dict[key] ).to(torch.float32)\n",
    "           \n",
    "# Create instances of TSDataset for each dataset\n",
    "datasets = { name: (TSDataset(data['data'], data['labels'], max_len=max_seq_len, shuffle=True) if type(data)==dict\n",
    "          else ImputationDataset(data, masking_ratio=0.25)) for name, data in data_dict.items() }\n",
    "\n",
    "# Create an instance of the custom data loader\n",
    "ts_data_loader = TSDataLoader(datasets, batch_size=512, max_len=max_seq_len, collate_fn='unsuperv', shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m      7\u001b[0m PROJECTION_DIMS         \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m     10\u001b[0m encoder_configs         \u001b[39m=\u001b[39m Configs(TSlength_aligned\u001b[39m=\u001b[39mmax_seq_len, \n\u001b[1;32m     11\u001b[0m                                     features_len\u001b[39m=\u001b[39mPROJECTION_DIMS, \n\u001b[1;32m     12\u001b[0m                                     features_len_f\u001b[39m=\u001b[39mPROJECTION_DIMS, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                     freeze\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m                                 )\n\u001b[0;32m---> 27\u001b[0m tsfm                    \u001b[39m=\u001b[39m TSFM(input_data_shapes_dict, \n\u001b[1;32m     28\u001b[0m                                 device\u001b[39m=\u001b[39;49mDEVICE,\n\u001b[1;32m     29\u001b[0m                                 max_seq_length\u001b[39m=\u001b[39;49mmax_seq_len,\n\u001b[1;32m     30\u001b[0m                                 encoder_config\u001b[39m=\u001b[39;49mencoder_configs,\n\u001b[1;32m     31\u001b[0m                                 projection_layer_dims\u001b[39m=\u001b[39;49mPROJECTION_DIMS,\n\u001b[1;32m     32\u001b[0m                                 )\n",
      "File \u001b[0;32m~/TS-FM/src/TSFM.py:87\u001b[0m, in \u001b[0;36mTSFM.__init__\u001b[0;34m(self, input_data_shapes_dict, projection_layer_encoder, encoder_layer, projection_layer_dims, depth, device, dtype, max_seq_length, encoder_config, univariate_forcast_hidden_dim, use_revin, univariate_criterion)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigs[dataset_name] \u001b[39m=\u001b[39m {}\n\u001b[1;32m     86\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize the encoder\"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_encoder_layer()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)  \u001b[39m#Encoder(encoder_layer, encoder_layer_dims, depth).to(self.device)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mswa_utils\u001b[39m.\u001b[39mAveragedModel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mupdate_parameters(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encoder)\n",
      "File \u001b[0;32m~/TS-FM/src/TSFM.py:364\u001b[0m, in \u001b[0;36mTSFM.get_encoder_layer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_encoder_layer\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTFC\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 364\u001b[0m         encoder \u001b[39m=\u001b[39m TFC(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_config)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    365\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEncoder layer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer\u001b[39m}\u001b[39;00m\u001b[39m is not implemented.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/TS-FM/src/encoders.py:20\u001b[0m, in \u001b[0;36mTFC.__init__\u001b[0;34m(self, configs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, configs):\n\u001b[1;32m     14\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    Initialize the TFC module.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m        configs: Configuration object containing the model parameters.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[39msuper\u001b[39;49m(TFC, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     22\u001b[0m     \u001b[39m# encoder_layers_t = TransformerEncoderLayer(configs.TSlength_aligned, dim_feedforward=configs.dim_feedforward, nhead=configs.n_head, batch_first=configs.batch_first)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39m# self.transformer_encoder_t = TransformerEncoder(encoder_layers_t, configs.num_transformer_layers)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder_t \u001b[39m=\u001b[39m TSTransformerEncoder(configs\u001b[39m.\u001b[39mfeatures_len, configs\u001b[39m.\u001b[39mTSlength_aligned, configs\u001b[39m.\u001b[39md_model, configs\u001b[39m.\u001b[39mn_head,\n\u001b[1;32m     26\u001b[0m                                     configs\u001b[39m.\u001b[39mnum_transformer_layers, configs\u001b[39m.\u001b[39mdim_feedforward, dropout\u001b[39m=\u001b[39mconfigs\u001b[39m.\u001b[39mdropout,\n\u001b[1;32m     27\u001b[0m                                     pos_encoding\u001b[39m=\u001b[39mconfigs\u001b[39m.\u001b[39mpos_encoding, activation\u001b[39m=\u001b[39mconfigs\u001b[39m.\u001b[39mtransformer_activation,\n\u001b[1;32m     28\u001b[0m                                     norm\u001b[39m=\u001b[39mconfigs\u001b[39m.\u001b[39mtransformer_normalization_layer, freeze\u001b[39m=\u001b[39mconfigs\u001b[39m.\u001b[39mfreeze\n\u001b[1;32m     29\u001b[0m                                     )\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "input_data_shapes_dict  = {name: data['data'].shape[1:] if type(data)==dict else data.shape[1:] for name, data in data_dict.items()}\n",
    "\n",
    "\n",
    "DEVICE                  = 'cuda'\n",
    "MAX_SEQ_LENGTH          = max_seq_len\n",
    "ENCODER_LAYER_DIMS      = 64\n",
    "PROJECTION_DIMS         = 128\n",
    "\n",
    "\n",
    "encoder_configs         = Configs(TSlength_aligned=max_seq_len, \n",
    "                                    features_len=PROJECTION_DIMS, \n",
    "                                    features_len_f=PROJECTION_DIMS, \n",
    "                                    encoder_layer_dims=ENCODER_LAYER_DIMS,\n",
    "                                    dim_feedforward=128,\n",
    "                                    linear_encoder_dim=256,\n",
    "                                    channel_output_size=10,\n",
    "                                    time_output_size=10,\n",
    "                                    d_model=128,\n",
    "                                    num_transformer_layers=1,\n",
    "                                    n_head=1,\n",
    "                                    pos_encoding='learnable',\n",
    "                                    transformer_activation='gelu',\n",
    "                                    transformer_normalization_layer='BatchNorm',\n",
    "                                    freeze=False,\n",
    "                                )\n",
    "\n",
    "tsfm                    = TSFM(input_data_shapes_dict, \n",
    "                                device=DEVICE,\n",
    "                                max_seq_length=max_seq_len,\n",
    "                                encoder_config=encoder_configs,\n",
    "                                projection_layer_dims=PROJECTION_DIMS,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_config_kwargs = {\n",
    "    \"dataset_btc\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_btc']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_btc']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_eth\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_eth']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_eth']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_xmr\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_xmr']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_xmr']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }, \n",
    "    \"univariate\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['univariate']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['univariate']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# TODO: Add learning rate to warmup config kwargs\n",
    "\n",
    "N_EPOCHS                 = 10\n",
    "WARMUP_EPOCHS            = 10\n",
    "WARMUP_BATCH_SIZE        = 128\n",
    "WARMUP_PROJECTION_LAYERS = False\n",
    "BATCH_SIZE               = 512\n",
    "LR                       = 1e-1\n",
    "LOG                      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 221054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss          = tsfm.fit(data_dict, n_epochs=N_EPOCHS, warmup_projection_layers=WARMUP_PROJECTION_LAYERS, \n",
    "                         log=LOG, verbose=True, shuffle=True, warmup_epochs=WARMUP_EPOCHS, \n",
    "                         warmup_config_kwargs=warmup_config_kwargs, warmup_batch_size=WARMUP_BATCH_SIZE,\n",
    "                         batch_size=BATCH_SIZE, lr=LR, device=DEVICE, max_seq_length=MAX_SEQ_LENGTH, \n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
