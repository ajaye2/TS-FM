{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchinfo\n",
    "# !pip install lumnisfactors\n",
    "# !pip install matplotlib\n",
    "# !pip install torchmetrics\n",
    "# !conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/baseline/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import grequests\n",
    "\n",
    "from src.mvts_transformer.ts_transformer import TSTransformerEncoder, model_factory\n",
    "from src.utils import create_3d_array, standardize, rolling_mean_diff, generate_univariate_data_labels, generate_data_labels_from_3d_array\n",
    "from src.projection_layers import LSTMMaskedAutoencoderProjection\n",
    "from src.dataset import TSDataset, ImputationDataset\n",
    "from src.dataloader import TSDataLoader\n",
    "from src.TFC.dataloader import TFCDataset\n",
    "from src.encoders import TFC\n",
    "from src.configs import Configs\n",
    "from src.RevIN import RevIN\n",
    "from src.TSFM import TSFM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.1+cu117\n",
      "CUDA available: True\n",
      "Torch version: 1.13.1+cu117\n",
      "CUDA available: True\n",
      "cuDNN version: 8401\n"
     ]
    }
   ],
   "source": [
    "# !conda uninstall pytorch torchvision -y\n",
    "# !pip install torch torchvision -f https://download.pytorch.org/whl/cu111/torch_stable.html\n",
    "import torch\n",
    "try:\n",
    "    !unset LD_LIBRARY_PATH\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "except:\n",
    "    !unset LD_LIBRARY_PATH\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumnisfactors import LumnisFactors\n",
    "from KEYS import LUMNIS_API_KEY\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "factorName          = \"price\"\n",
    "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
    "path_to_data = \"/home/ec2-user/TS-FM/src/data/\"\n",
    "\n",
    "btc_file = Path(path_to_data + \"btc.csv\")\n",
    "eth_file = Path(path_to_data + \"eth.csv\")\n",
    "xmr_file = Path(path_to_data + \"xmr.csv\")\n",
    "\n",
    "if btc_file.is_file():\n",
    "    temp_df_btc_raw     = pd.read_csv(path_to_data + \"btc.csv\").set_index(\"Unnamed: 0\")\n",
    "else:\n",
    "    temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "    temp_df_btc_raw.to_csv(path_to_data + \"btc.csv\")\n",
    "\n",
    "if eth_file.is_file():\n",
    "    temp_df_eth_raw     = pd.read_csv(path_to_data + \"eth.csv\").set_index(\"Unnamed: 0\")\n",
    "else:\n",
    "    temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "    temp_df_eth_raw.to_csv(path_to_data + \"eth.csv\")\n",
    "\n",
    "if xmr_file.is_file():\n",
    "    temp_df_xmr_raw     = pd.read_csv(path_to_data + \"xmr.csv\").set_index(\"Unnamed: 0\")\n",
    "else:\n",
    "    temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "    temp_df_xmr_raw.to_csv(path_to_data + \"xmr.csv\")\n",
    "\n",
    "# ob_df_raw           = lumnis.get_historical_data(\"orderbook_snapshot_5\", \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_btc         = rolling_mean_diff(temp_df_btc_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_eth         = rolling_mean_diff(temp_df_eth_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_xmr         = rolling_mean_diff(temp_df_xmr_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "\n",
    "cols                = temp_df_btc.columns #['close', 'volume'] #\n",
    "max_seq_len         = 150\n",
    "\n",
    "btc_array           = create_3d_array(temp_df_btc[cols], temp_df_btc.index, max_seq_len)\n",
    "eth_array           = create_3d_array(temp_df_eth[cols], temp_df_eth.index, max_seq_len)\n",
    "xmr_array           = create_3d_array(temp_df_xmr[cols], temp_df_xmr.index, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14515, 150, 104) (1814, 150, 104) (1815, 150, 104)\n"
     ]
    }
   ],
   "source": [
    "btc_array.shape\n",
    "\n",
    "def get_train_val_test_array(array, train_size, val_size, test_size):\n",
    "    train_len          = int(len(array)*train_size)\n",
    "    val_len            = int(len(array)*val_size)\n",
    "    test_len           = int(len(array)*test_size)\n",
    "\n",
    "    train_array, val_array, test_array = array[:train_len], array[train_len:train_len+val_len], array[train_len+val_len:]\n",
    "    return train_array, val_array, test_array\n",
    "\n",
    "btc_train_array, btc_val_array, btc_test_array = get_train_val_test_array(btc_array, 0.8, 0.1, 0.1)\n",
    "eth_train_array, eth_val_array, eth_test_array = get_train_val_test_array(eth_array, 0.8, 0.1, 0.1)\n",
    "xmr_train_array, xmr_val_array, xmr_test_array = get_train_val_test_array(xmr_array, 0.8, 0.1, 0.1)\n",
    "\n",
    "\n",
    "print(btc_train_array.shape, btc_val_array.shape, btc_test_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56762, 150, 1) (56762, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "univariate_array_eth         = create_3d_array(temp_df_eth_raw[['close']], temp_df_eth_raw.index, max_seq_len)\n",
    "univariate_array_btc         = create_3d_array(temp_df_btc_raw[['close']], temp_df_btc_raw.index, max_seq_len)\n",
    "univariate_array_xmr         = create_3d_array(temp_df_xmr_raw[['close']], temp_df_xmr_raw.index, max_seq_len)\n",
    "\n",
    "uni_data_eth, uni_labels_eth = generate_univariate_data_labels(univariate_array_eth)\n",
    "uni_data_btc, uni_labels_btc = generate_univariate_data_labels(univariate_array_btc)\n",
    "uni_data_xmr, uni_labels_xmr = generate_univariate_data_labels(univariate_array_xmr)\n",
    "\n",
    "uni_data                     = np.concatenate((uni_data_eth, uni_data_btc, uni_data_xmr), axis=0)\n",
    "uni_labels                   = np.concatenate((uni_labels_eth, uni_labels_btc, uni_labels_xmr), axis=0)\n",
    "\n",
    "print(uni_data.shape, uni_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btc, labels_btc = generate_data_labels_from_3d_array(btc_array)\n",
    "data_eth, labels_eth = generate_data_labels_from_3d_array(eth_array)\n",
    "data_xmr, labels_xmr = generate_data_labels_from_3d_array(xmr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data as a dictionary\n",
    "data_dict = {\n",
    "    # 'univariate': ,#{\"data\": uni_data, \"labels\": uni_labels},\n",
    "    'dataset_btc': btc_train_array,#{'data': data_btc, 'labels': labels_btc},\n",
    "    'dataset_eth': eth_train_array,#{'data': data_eth, 'labels': labels_eth},\n",
    "    'dataset_xmr': xmr_train_array, #{'data': data_xmr, 'labels': labels_xmr},#xmr_array,\n",
    "    \n",
    "}\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "for key in data_dict.keys():\n",
    "    if type(data_dict[key]) == dict: \n",
    "        data_dict[key]['data'] = torch.from_numpy( data_dict[key]['data'] ).to(torch.float32)\n",
    "        data_dict[key]['labels'] = torch.from_numpy( data_dict[key]['labels'] ).to(torch.float32)\n",
    "    else:\n",
    "        data_dict[key] = torch.from_numpy( data_dict[key] ).to(torch.float32)\n",
    "           \n",
    "# Create instances of TSDataset for each dataset\n",
    "datasets = { name: (TSDataset(data['data'], data['labels'], max_len=max_seq_len, shuffle=True) if type(data)==dict\n",
    "          else ImputationDataset(data, masking_ratio=0.25)) for name, data in data_dict.items() }\n",
    "\n",
    "# Create an instance of the custom data loader\n",
    "ts_data_loader = TSDataLoader(datasets, batch_size=512, max_len=max_seq_len, collate_fn='unsuperv', shuffle=False)\n",
    "\n",
    "#Takes 6 mins to load 43371 samples with 150 timesteps each, and 104 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shapes_dict  = {name: data['data'].shape[1:] if type(data)==dict else data.shape[1:] for name, data in data_dict.items()}\n",
    "# input_data_shapes_dict = {\"temp\": (max_seq_len, 104)}\n",
    "\n",
    "DEVICE                  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_SEQ_LENGTH          = max_seq_len\n",
    "ENCODER_LAYER_DIMS      = 64\n",
    "PROJECTION_DIMS         = 128\n",
    "\n",
    "\n",
    "encoder_configs         = Configs(TSlength_aligned=max_seq_len, \n",
    "                                    features_len=PROJECTION_DIMS, \n",
    "                                    features_len_f=PROJECTION_DIMS, \n",
    "                                    encoder_layer_dims=ENCODER_LAYER_DIMS,\n",
    "                                    dim_feedforward=128,\n",
    "                                    linear_encoder_dim=256,\n",
    "                                    channel_output_size=10,\n",
    "                                    time_output_size=10,\n",
    "                                    d_model=128,\n",
    "                                    num_transformer_layers=1,\n",
    "                                    n_head=1,\n",
    "                                    pos_encoding='learnable',\n",
    "                                    transformer_activation='gelu',\n",
    "                                    transformer_normalization_layer='BatchNorm',\n",
    "                                    freeze=False,\n",
    "                                    device=DEVICE,\n",
    "                                )\n",
    "\n",
    "tsfm                    = TSFM(input_data_shapes_dict, \n",
    "                                model_name=\"INIT_TEST\",\n",
    "                                device=DEVICE,\n",
    "                                max_seq_length=max_seq_len,\n",
    "                                encoder_config=encoder_configs,\n",
    "                                projection_layer_dims=PROJECTION_DIMS,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_config_kwargs = {\n",
    "    \"dataset_btc\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_btc']['data'].shape[-1] if type(data_dict['dataset_btc'])==dict else data_dict['dataset_btc'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_btc']['data'].shape[1] if type(data_dict['dataset_btc'])==dict else data_dict['dataset_btc'].shape[1],\n",
    "        \"data_set_type\": ImputationDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-4,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_eth\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_eth']['data'].shape[-1] if type(data_dict['dataset_eth'])==dict else data_dict['dataset_eth'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_eth']['data'].shape[1]  if type(data_dict['dataset_eth'])==dict else data_dict['dataset_eth'].shape[1],\n",
    "        \"data_set_type\": ImputationDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-4,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_xmr\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_xmr']['data'].shape[-1] if type(data_dict['dataset_xmr'])==dict else data_dict['dataset_xmr'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_xmr']['data'].shape[1] if type(data_dict['dataset_xmr'])==dict else data_dict['dataset_xmr'].shape[1],\n",
    "        \"data_set_type\": ImputationDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-4,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    # \"univariate\": {\n",
    "    #     \"batch_size\": 512,\n",
    "    #     \"input_channels\": data_dict['univariate']['data'].shape[-1],\n",
    "    #     \"timesteps\": data_dict['univariate']['data'].shape[1],\n",
    "    #     \"data_set_type\": TSDataset,\n",
    "    #     \"num_epochs\": 30,\n",
    "    #     \"lr\": 1e-4,\n",
    "    #     \"kwargs\": {\n",
    "    #         \"verbose\": False,\n",
    "    #     }\n",
    "    # }\n",
    "}\n",
    "# TODO: Add learning rate to warmup config kwargs\n",
    "\n",
    "N_EPOCHS                 = 10\n",
    "WARMUP_EPOCHS            = 30\n",
    "WARMUP_BATCH_SIZE        = 512\n",
    "WARMUP_PROJECTION_LAYERS = True\n",
    "BATCH_SIZE               = 512\n",
    "LR                       = 1e-4\n",
    "LOG                      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 43371\n",
      "Warming up with 28 batches of size 512. Dataset name dataset_btc.\n",
      "Epoch: 0, Loss: 0.681320322411401\n",
      "Epoch: 1, Loss: 0.6668092054980141\n",
      "Epoch: 2, Loss: 0.6282743364572525\n",
      "Epoch: 3, Loss: 0.5819922941071647\n",
      "Epoch: 4, Loss: 0.5357974746397564\n",
      "Epoch: 5, Loss: 0.4838854819536209\n",
      "Epoch: 6, Loss: 0.42434598611933844\n",
      "Epoch: 7, Loss: 0.37400681312595097\n",
      "Epoch: 8, Loss: 0.33761314089809147\n",
      "Epoch: 9, Loss: 0.309508961226259\n",
      "Epoch: 10, Loss: 0.2893134302326611\n",
      "Epoch: 11, Loss: 0.27447117545775007\n",
      "Epoch: 12, Loss: 0.2641363133277212\n",
      "Epoch: 13, Loss: 0.2561964680041586\n",
      "Epoch: 14, Loss: 0.2501043368663107\n",
      "Epoch: 15, Loss: 0.24508238477366312\n",
      "Epoch: 16, Loss: 0.24033734574913979\n",
      "Epoch: 17, Loss: 0.2358790369970458\n",
      "Epoch: 18, Loss: 0.23206295924527304\n",
      "Epoch: 19, Loss: 0.22846026558961188\n",
      "Epoch: 20, Loss: 0.2253107162458556\n",
      "Epoch: 21, Loss: 0.2225188479891845\n",
      "Epoch: 22, Loss: 0.2194064413862569\n",
      "Epoch: 23, Loss: 0.2168084941804409\n",
      "Epoch: 24, Loss: 0.21460471940892084\n",
      "Epoch: 25, Loss: 0.21228656811373575\n",
      "Epoch: 26, Loss: 0.21002599543758801\n",
      "Epoch: 27, Loss: 0.20806860976985522\n",
      "Epoch: 28, Loss: 0.2061235665210656\n",
      "Epoch: 29, Loss: 0.20409028923937253\n",
      "Warming up with 28 batches of size 512. Dataset name dataset_eth.\n",
      "Epoch: 0, Loss: 0.669742328780038\n",
      "Epoch: 1, Loss: 0.6602440518992287\n",
      "Epoch: 2, Loss: 0.630868952189173\n",
      "Epoch: 3, Loss: 0.5817509293556213\n",
      "Epoch: 4, Loss: 0.5264110118150711\n",
      "Epoch: 5, Loss: 0.46487771826131\n",
      "Epoch: 6, Loss: 0.40167140534945894\n",
      "Epoch: 7, Loss: 0.35875758848020006\n",
      "Epoch: 8, Loss: 0.32914168387651443\n",
      "Epoch: 9, Loss: 0.3056802877358028\n",
      "Epoch: 10, Loss: 0.2864899752395494\n",
      "Epoch: 11, Loss: 0.2715456858277321\n",
      "Epoch: 12, Loss: 0.2608884572982788\n",
      "Epoch: 13, Loss: 0.25307777523994446\n",
      "Epoch: 14, Loss: 0.24778104573488235\n",
      "Epoch: 15, Loss: 0.24312853600297654\n",
      "Epoch: 16, Loss: 0.23931010706084116\n",
      "Epoch: 17, Loss: 0.23583027135048593\n",
      "Epoch: 18, Loss: 0.2324860915541649\n",
      "Epoch: 19, Loss: 0.22945127316883632\n",
      "Epoch: 20, Loss: 0.22672216966748238\n",
      "Epoch: 21, Loss: 0.22376679256558418\n",
      "Epoch: 22, Loss: 0.22089255175420217\n",
      "Epoch: 23, Loss: 0.21843764558434486\n",
      "Epoch: 24, Loss: 0.2157974833888667\n",
      "Epoch: 25, Loss: 0.2134660854935646\n",
      "Epoch: 26, Loss: 0.211179254842656\n",
      "Epoch: 27, Loss: 0.20960765225546701\n",
      "Epoch: 28, Loss: 0.20745837369135448\n",
      "Epoch: 29, Loss: 0.20578742931996072\n",
      "Warming up with 28 batches of size 512. Dataset name dataset_xmr.\n",
      "Epoch: 0, Loss: 0.6735768382038388\n",
      "Epoch: 1, Loss: 0.6601727902889252\n",
      "Epoch: 2, Loss: 0.6308035829237529\n",
      "Epoch: 3, Loss: 0.590936792748315\n",
      "Epoch: 4, Loss: 0.5466764718294144\n",
      "Epoch: 5, Loss: 0.497154352920396\n",
      "Epoch: 6, Loss: 0.43560936621257235\n",
      "Epoch: 7, Loss: 0.37754616141319275\n",
      "Epoch: 8, Loss: 0.3385519566280501\n",
      "Epoch: 9, Loss: 0.3104231410792896\n",
      "Epoch: 10, Loss: 0.29029117950371336\n",
      "Epoch: 11, Loss: 0.2762807554432324\n",
      "Epoch: 12, Loss: 0.26661705119269236\n",
      "Epoch: 13, Loss: 0.2597849454198565\n",
      "Epoch: 14, Loss: 0.25372761328305515\n",
      "Epoch: 15, Loss: 0.24836142946566855\n",
      "Epoch: 16, Loss: 0.24374936574271747\n",
      "Epoch: 17, Loss: 0.23992533875363214\n",
      "Epoch: 18, Loss: 0.23634425976446696\n",
      "Epoch: 19, Loss: 0.23302943419132913\n",
      "Epoch: 20, Loss: 0.23007792606949806\n",
      "Epoch: 21, Loss: 0.2277428741965975\n",
      "Epoch: 22, Loss: 0.22496730302061355\n",
      "Epoch: 23, Loss: 0.22247522324323654\n",
      "Epoch: 24, Loss: 0.22025445848703384\n",
      "Epoch: 25, Loss: 0.2181375984634672\n",
      "Epoch: 26, Loss: 0.2163389309176377\n",
      "Epoch: 27, Loss: 0.21398096212318965\n",
      "Epoch: 28, Loss: 0.21208327848996436\n",
      "Epoch: 29, Loss: 0.21044924961669104\n",
      "Epoch #0, Iter #0: loss=12.083614349365234 for dataset_btc\n",
      "Epoch #0, Iter #0: loss=11.891592025756836 for dataset_eth\n",
      "Epoch #0, Iter #0: loss=11.678006172180176 for dataset_xmr\n",
      "Epoch #0: loss=3.282425398114084 for dataset_btc\n",
      "Epoch #0: loss=3.2341171790813577 for dataset_eth\n",
      "Epoch #0: loss=3.15100115195088 for dataset_xmr\n",
      "Epoch #1: loss=2.4554487699749825 for dataset_btc\n",
      "Epoch #1: loss=2.407640172147203 for dataset_eth\n",
      "Epoch #1: loss=2.4361496519768373 for dataset_xmr\n",
      "Epoch #2: loss=2.119710653677754 for dataset_btc\n",
      "Epoch #2: loss=2.1549688865398537 for dataset_eth\n",
      "Epoch #2: loss=2.1678934535760988 for dataset_xmr\n",
      "Epoch #3: loss=2.008377415010299 for dataset_btc\n",
      "Epoch #3: loss=2.0482101495238556 for dataset_eth\n",
      "Epoch #3: loss=2.0503955819140907 for dataset_xmr\n",
      "Epoch #4: loss=1.9545044460515866 for dataset_btc\n",
      "Epoch #4: loss=1.9850531282096073 for dataset_eth\n",
      "Epoch #4: loss=1.9873339335123699 for dataset_xmr\n",
      "Epoch #5: loss=1.915113038030164 for dataset_btc\n",
      "Epoch #5: loss=1.9355828460605664 for dataset_eth\n",
      "Epoch #5: loss=1.939181026371046 for dataset_xmr\n",
      "Epoch #6: loss=1.884190734775587 for dataset_btc\n",
      "Epoch #6: loss=1.8971309058967678 for dataset_eth\n",
      "Epoch #6: loss=1.8960270388373013 for dataset_xmr\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss          = tsfm.fit(data_dict, n_epochs=N_EPOCHS, warmup_projection_layers=WARMUP_PROJECTION_LAYERS, \n",
    "                         log=LOG, verbose=True, shuffle=True, warmup_epochs=WARMUP_EPOCHS, \n",
    "                         warmup_config_kwargs=warmup_config_kwargs, warmup_batch_size=WARMUP_BATCH_SIZE,\n",
    "                         batch_size=BATCH_SIZE, lr=LR, device=DEVICE, max_seq_length=MAX_SEQ_LENGTH, \n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
