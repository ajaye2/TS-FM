{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchinfo\n",
    "# !pip install lumnisfactors\n",
    "# !pip install matplotlib\n",
    "# !pip install torchmetrics\n",
    "# !conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import grequests\n",
    "\n",
    "from src.mvts_transformer.ts_transformer import TSTransformerEncoder, model_factory\n",
    "from src.utils import create_3d_array, standardize, rolling_mean_diff, generate_univariate_data_labels, generate_data_labels_from_3d_array\n",
    "from src.projection_layers import LSTMMaskedAutoencoderProjection\n",
    "from src.dataset import TSDataset, ImputationDataset\n",
    "from src.dataloader import TSDataLoader\n",
    "from src.TFC.dataloader import TFCDataset\n",
    "from src.encoders import TFC\n",
    "from src.configs import Configs\n",
    "from src.RevIN import RevIN\n",
    "from src.TSFM import TSFM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumnisfactors import LumnisFactors\n",
    "from KEYS import LUMNIS_API_KEY\n",
    "\n",
    "factorName          = \"price\"\n",
    "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
    "temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "ob_df_raw           = lumnis.get_historical_data(\"orderbook_snapshot_5\", \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_btc         = rolling_mean_diff(temp_df_btc_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_eth         = rolling_mean_diff(temp_df_eth_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_xmr         = rolling_mean_diff(temp_df_xmr_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "\n",
    "\n",
    "cols                = temp_df_btc.columns #['close', 'volume'] #\n",
    "max_seq_len         = 150\n",
    "\n",
    "btc_array           = create_3d_array(temp_df_btc[cols], temp_df_btc.index, max_seq_len)\n",
    "eth_array           = create_3d_array(temp_df_eth[cols], temp_df_eth.index, max_seq_len)\n",
    "xmr_array           = create_3d_array(temp_df_xmr[cols], temp_df_xmr.index, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56762, 150, 1) (56762, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "univariate_array_eth         = create_3d_array(temp_df_eth_raw[['close']], temp_df_eth_raw.index, max_seq_len)\n",
    "univariate_array_btc         = create_3d_array(temp_df_btc_raw[['close']], temp_df_btc_raw.index, max_seq_len)\n",
    "univariate_array_xmr         = create_3d_array(temp_df_xmr_raw[['close']], temp_df_xmr_raw.index, max_seq_len)\n",
    "\n",
    "uni_data_eth, uni_labels_eth = generate_univariate_data_labels(univariate_array_eth)\n",
    "uni_data_btc, uni_labels_btc = generate_univariate_data_labels(univariate_array_btc)\n",
    "uni_data_xmr, uni_labels_xmr = generate_univariate_data_labels(univariate_array_xmr)\n",
    "\n",
    "uni_data                     = np.concatenate((uni_data_eth, uni_data_btc, uni_data_xmr), axis=0)\n",
    "uni_labels                   = np.concatenate((uni_labels_eth, uni_labels_btc, uni_labels_xmr), axis=0)\n",
    "\n",
    "print(uni_data.shape, uni_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_btc, labels_btc = generate_data_labels_from_3d_array(btc_array)\n",
    "data_eth, labels_eth = generate_data_labels_from_3d_array(eth_array)\n",
    "data_xmr, labels_xmr = generate_data_labels_from_3d_array(xmr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data as a dictionary\n",
    "data_dict = {\n",
    "    'dataset_btc': btc_array,\n",
    "    'dataset_eth': eth_array,\n",
    "    'dataset_xmr': xmr_array,\n",
    "    'univariate': {\"data\": uni_data, \"labels\": uni_labels}\n",
    "}\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "for key in data_dict.keys():\n",
    "    if key == 'univariate': \n",
    "        data_dict[key]['data']   = torch.from_numpy( data_dict[key]['data'] ).to(torch.float32)\n",
    "        data_dict[key]['labels'] = torch.from_numpy( data_dict[key]['labels'] ).to(torch.float32)\n",
    "    else:\n",
    "        data_dict[key] = torch.from_numpy( data_dict[key] ).to(torch.float32)\n",
    "        \n",
    "# Create instances of TSDataset for each dataset\n",
    "datasets = { name: (TSDataset(data['data'], data['labels'], max_len=max_seq_len, shuffle=True) if name=='univariate'\n",
    "          else ImputationDataset(data, masking_ratio=0.25)) for name, data in data_dict.items() }\n",
    "\n",
    "# Create an instance of the custom data loader\n",
    "ts_data_loader = TSDataLoader(datasets, batch_size=512, max_len=max_seq_len, collate_fn='unsuperv', shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shapes_dict  = {name: data['data'].shape[1:] if name=='univariate' else data.shape[1:] for name, data in data_dict.items()}\n",
    "\n",
    "\n",
    "BATCH_SIZE              = 128\n",
    "LR                      = 1e-1\n",
    "LOG                     = True\n",
    "DEVICE                  = 'cuda'\n",
    "MAX_SEQ_LENGTH          = max_seq_len\n",
    "ENCODER_LAYER_DIMS      = 128\n",
    "PROJECTION_DIMS         = 128\n",
    "\n",
    "\n",
    "encoder_configs         = Configs(TSlength_aligned=max_seq_len, \n",
    "                                    features_len=PROJECTION_DIMS, \n",
    "                                    features_len_f=PROJECTION_DIMS, \n",
    "                                    encoder_layer_dims=ENCODER_LAYER_DIMS,\n",
    "                                    dim_feedforward=128,\n",
    "                                    linear_encoder_dim=256,\n",
    "                                    channel_output_size=10,\n",
    "                                    time_output_size=10,\n",
    "                                    d_model=128,\n",
    "                                    num_transformer_layers=1,\n",
    "                                    n_head=1,\n",
    "                                    pos_encoding='learnable',\n",
    "                                    transformer_activation='gelu',\n",
    "                                    transformer_normalization_layer='BatchNorm',\n",
    "                                    freeze=False,\n",
    "                                )\n",
    "\n",
    "tsfm                    = TSFM(input_data_shapes_dict, \n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                lr=1e-3, \n",
    "                                log=True, \n",
    "                                device='cuda',\n",
    "                                max_seq_length=max_seq_len,\n",
    "                                encoder_config=encoder_configs,\n",
    "                                projection_layer_dims=PROJECTION_DIMS,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 167739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/TS-FM/src/TSFM.py:196: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  train_data      = train_data[mask, :].astype(np.float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m WARMUP_BATCH_SIZE        \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m\n\u001b[1;32m     47\u001b[0m WARMUP_PROJECTION_LAYERS \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m loss          \u001b[39m=\u001b[39m tsfm\u001b[39m.\u001b[39;49mfit(data_dict, n_epochs\u001b[39m=\u001b[39;49mN_EPOCHS, warmup_projection_layers\u001b[39m=\u001b[39;49mWARMUP_PROJECTION_LAYERS, \n\u001b[1;32m     50\u001b[0m                          log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, warmup_epochs\u001b[39m=\u001b[39;49mWARMUP_EPOCHS, \n\u001b[1;32m     51\u001b[0m                          warmup_config_kwargs\u001b[39m=\u001b[39;49mwarmup_config_kwargs, warmup_batch_size\u001b[39m=\u001b[39;49mWARMUP_BATCH_SIZE\n\u001b[1;32m     52\u001b[0m                         )\n",
      "File \u001b[0;32m~/TS-FM/src/TSFM.py:128\u001b[0m, in \u001b[0;36mTSFM.fit\u001b[0;34m(self, train_data_dict, labels, n_epochs, n_iters, verbose, shuffle, warmup_projection_layers, warmup_epochs, log, subset, configs, training_mode, warmup_config_kwargs, warmup_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal number of data points: \u001b[39m\u001b[39m{\u001b[39;00mtotal_number_of_data_points\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Warmup the projection layers\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m datasets, optimizer_list, encoder_dataset_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarmup(train_data_dict, warmup_projection_layers\u001b[39m=\u001b[39;49mwarmup_projection_layers, \n\u001b[1;32m    129\u001b[0m                                                              warmup_epochs\u001b[39m=\u001b[39;49mwarmup_epochs, shuffle\u001b[39m=\u001b[39;49mshuffle, \n\u001b[1;32m    130\u001b[0m                                                              warmup_config_kwargs\u001b[39m=\u001b[39;49mwarmup_config_kwargs, warmup_batch_size\u001b[39m=\u001b[39;49mwarmup_batch_size, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Disable revIN for univariate forcasting\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39munivariate\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_layers\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/TS-FM/src/TSFM.py:196\u001b[0m, in \u001b[0;36mTSFM.warmup\u001b[0;34m(self, train_data_dict, labels, n_epochs, n_iters, verbose, shuffle, warmup_projection_layers, warmup_epochs, log, subset, configs, training_mode, warmup_config_kwargs, warmup_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Remove the nan values\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m mask            \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mnp\u001b[39m.\u001b[39misnan(train_data)\u001b[39m.\u001b[39mall(axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mall(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 196\u001b[0m train_data      \u001b[39m=\u001b[39m train_data[mask, :]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    197\u001b[0m \u001b[39m# train_data             = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert the data to torch\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "warmup_config_kwargs = {\n",
    "    \"dataset_btc\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_btc'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_btc'].shape[1],\n",
    "        \"data_set_type\": ImputationDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_eth\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_eth'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_eth'].shape[1],\n",
    "        \"data_set_type\": ImputationDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_xmr\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_xmr'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_xmr'].shape[1],\n",
    "        \"data_set_type\": ImputationDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }, \n",
    "    \"univariate\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['univariate']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['univariate']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "N_EPOCHS                 = 10\n",
    "WARMUP_EPOCHS            = 10\n",
    "WARMUP_BATCH_SIZE        = 128\n",
    "WARMUP_PROJECTION_LAYERS = True\n",
    "\n",
    "loss          = tsfm.fit(data_dict, n_epochs=N_EPOCHS, warmup_projection_layers=WARMUP_PROJECTION_LAYERS, \n",
    "                         log=True, verbose=True, shuffle=True, warmup_epochs=WARMUP_EPOCHS, \n",
    "                         warmup_config_kwargs=warmup_config_kwargs, warmup_batch_size=WARMUP_BATCH_SIZE\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
