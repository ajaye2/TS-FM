{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchinfo\n",
    "# !pip install lumnisfactors\n",
    "# !pip install matplotlib\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuj/opt/anaconda3/envs/baseline/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import grequests\n",
    "\n",
    "from src.mvts_transformer.ts_transformer import TSTransformerEncoder, model_factory\n",
    "from src.utils import create_3d_array, standardize, rolling_mean_diff\n",
    "from src.projection_layers import LSTMMaskedAutoencoderProjection\n",
    "from src.dataset import TSDataset, ImputationDataset\n",
    "from src.dataloader import TSDataLoader\n",
    "from src.TFC.dataloader import TFCDataset\n",
    "from src.TFC.model import TFC\n",
    "from src.configs import Configs\n",
    "from src.RevIN import RevIN\n",
    "from src.TSFM import TSFM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumnisfactors import LumnisFactors\n",
    "from KEYS import LUMNIS_API_KEY\n",
    "\n",
    "factorName          = \"price\"\n",
    "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
    "temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-13\")\n",
    "temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-13\")\n",
    "temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-13\")\n",
    "ob_df_raw           = lumnis.get_historical_data(\"orderbook_snapshot_5\", \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-13\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_df               = rolling_mean_diff(ob_df_raw, [5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000], 'standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_btc         = rolling_mean_diff(temp_df_btc_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_eth         = rolling_mean_diff(temp_df_eth_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_xmr         = rolling_mean_diff(temp_df_xmr_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "\n",
    "\n",
    "cols                = temp_df_btc.columns #['close', 'volume'] #\n",
    "max_seq_len         = 50\n",
    "\n",
    "btc_array           = create_3d_array(temp_df_btc[cols], temp_df_btc.index, max_seq_len)\n",
    "eth_array           = create_3d_array(temp_df_eth[cols], temp_df_eth.index, max_seq_len)\n",
    "xmr_array           = create_3d_array(temp_df_xmr[cols], temp_df_xmr.index, max_seq_len)\n",
    "\n",
    "# x_data_f = fft.fft(torch.from_numpy( temp_df_btc.values )).abs()\n",
    "# ( temp_df_btc_raw / temp_df_btc_raw.rolling(1000).max() )['volume'].plot()\n",
    "# standardize(temp_df_btc, 500, type='standard')['close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data as a dictionary\n",
    "data_dict = {\n",
    "    'dataset_btc': torch.from_numpy( btc_array).to(torch.float32),\n",
    "    'dataset_eth': torch.from_numpy( eth_array).to(torch.float32),\n",
    "    'dataset_xmr': torch.from_numpy( xmr_array).to(torch.float32)\n",
    "}\n",
    "\n",
    "# Create instances of TSDataset for each dataset\n",
    "datasets = {name: ImputationDataset(data) for name, data in data_dict.items()}\n",
    "\n",
    "# Create an instance of the custom data loader\n",
    "ts_data_loader = TSDataLoader(datasets, batch_size=64, max_len=max_seq_len, collate_fn='unsuperv')\n",
    "\n",
    "num_epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_btc torch.Size([64, 50, 104])\n",
      "dataset_eth torch.Size([64, 50, 104])\n",
      "dataset_xmr torch.Size([64, 50, 104])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_data in ts_data_loader:\n",
    "        for dataset_name, data in batch_data.items():\n",
    "            \n",
    "            ds_type = ts_data_loader.datasets[dataset_name]\n",
    "            if isinstance(ds_type, ImputationDataset):\n",
    "                inputs, targets, target_masks, padding_masks = data\n",
    "            elif isinstance(ds_type, TSDataset):\n",
    "                if len(data) == 2: inputs, labels = data\n",
    "                else: inputs = data\n",
    "\n",
    "            # print(dataset_name, inputs.shape)\n",
    "            print(dataset_name, inputs.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btc_array[:1].shape\n",
    "# nn.Conv1d(input_dims[1], 20, kernel_size=3, padding=1)(btc_array[:1].transpose(1,2))\n",
    "# nn.ConvTranspose1d("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "LSTMMaskedAutoencoderProjection          [128, 50, 104]            --\n",
      "├─RevIN: 1-1                             [128, 50, 104]            208\n",
      "├─LSTM: 1-2                              [128, 50, 16]             7,808\n",
      "├─LSTM: 1-3                              [128, 50, 16]             2,176\n",
      "├─Linear: 1-4                            [128, 50, 104]            1,768\n",
      "├─RevIN: 1-5                             [128, 50, 104]            (recursive)\n",
      "==========================================================================================\n",
      "Total params: 11,960\n",
      "Trainable params: 11,960\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 66.79\n",
      "==========================================================================================\n",
      "Input size (MB): 2.66\n",
      "Forward/backward pass size (MB): 17.61\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 20.32\n",
      "==========================================================================================\n",
      "Warming up with 141 batches of size 128\n",
      "Epoch: 0, Loss: 0.6464313205252302\n",
      "Epoch: 1, Loss: 0.29975373715373643\n",
      "Epoch: 2, Loss: 0.20594110218345696\n",
      "Epoch: 3, Loss: 0.17054981331453256\n",
      "Epoch: 4, Loss: 0.15427024863290448\n",
      "Epoch: 5, Loss: 0.14500405158556945\n",
      "Epoch: 6, Loss: 0.13830617503494236\n",
      "Epoch: 7, Loss: 0.1330645410410056\n",
      "Epoch: 8, Loss: 0.12981192381880807\n",
      "Epoch: 9, Loss: 0.12804884928549434\n"
     ]
    }
   ],
   "source": [
    "# Init parameters\n",
    "input_dims  = btc_array.shape[1:]\n",
    "hidden_dims = 16 \n",
    "output_dims = 16 \n",
    "\n",
    "# Create an instance of the model\n",
    "masked_ae = LSTMMaskedAutoencoderProjection(input_dims, hidden_dims, output_dims, device='cpu', use_revin=True, lose_type='masked_mse')\n",
    "\n",
    "# Print the model summary\n",
    "print( summary(masked_ae, (128,) + input_dims) )\n",
    "\n",
    "# Train the model\n",
    "masked_ae.warmup(dataset=datasets['dataset_btc'], max_len=max_seq_len, n_epochs=10, batch_size=128, learning_rate=1e-3, log=True, data_set_type=type( datasets['dataset_btc'] ), collate_fn='unsuperv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {}\n",
    "config_kwargs['batch_size']          = 128\n",
    "config_kwargs['input_channels']      = data_dict['dataset_btc'].shape[-1]\n",
    "config_kwargs['timesteps']           = data_dict['dataset_btc'].shape[1]\n",
    "config_kwargs['TSlength_aligned']    = max_seq_len\n",
    "\n",
    "\n",
    "configs = Configs(**config_kwargs)\n",
    "labels = torch.zeros((data_dict['dataset_btc'].shape[0], 1))\n",
    "ds = {\"samples\": data_dict['dataset_btc'], \"labels\": labels}\n",
    "\n",
    "tfc_ds = TFCDataset(ds, configs, \"pre_train\", target_dataset_size=configs.batch_size, subset=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'masked_ae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reconstructed_input \u001b[39m=\u001b[39m masked_ae(inputs, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m channel  \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      5\u001b[0m instance \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'masked_ae' is not defined"
     ]
    }
   ],
   "source": [
    "reconstructed_input = masked_ae(inputs, training=False)\n",
    "\n",
    "channel  = 100\n",
    "instance = 10\n",
    "\n",
    "for channel in range(0, 20):\n",
    "    plt.figure()\n",
    "    plt.plot(inputs[instance][:,channel].detach().numpy()) \n",
    "    plt.plot(reconstructed_input[instance][:,channel].detach().numpy()) \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "embeddings = masked_ae.encode(inputs, type_of_pooling='mean')\n",
    "# plt.plot( embeddings[:,2].detach().numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TFC.model import TFC\n",
    "configs = Configs(TSlength_aligned=50, \n",
    "                  features_len=inputs.shape[-1], \n",
    "                  features_len_f=inputs.shape[-1], \n",
    "                  n_head=1,\n",
    "                  dim_feedforward=128,\n",
    "                  num_transformer_layers=1,\n",
    "                  encoder_layer_dims=128,\n",
    "                  linear_encoder_dim=256,\n",
    "                  channel_output_size=10,\n",
    "                  time_output_size=10\n",
    "                  )\n",
    "tfc     = TFC(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tfc.encode(inputs.permute(0, 2, 1), inputs.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_time torch.Size([64, 100])\n",
      "z_time torch.Size([64, 128])\n",
      "h_freq torch.Size([64, 100])\n",
      "z_freq torch.Size([64, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TFC                                           [64, 100]                 --\n",
       "├─TransformerEncoder: 1-1                     [64, 104, 50]             --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [64, 104, 50]             23,378\n",
       "├─Sequential: 1-2                             [64, 100]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-2                 [64, 10, 10]              --\n",
       "│    └─Flatten: 2-3                           [64, 100]                 --\n",
       "├─Sequential: 1-3                             [64, 128]                 --\n",
       "│    └─Linear: 2-4                            [64, 256]                 25,856\n",
       "│    └─BatchNorm1d: 2-5                       [64, 256]                 512\n",
       "│    └─ReLU: 2-6                              [64, 256]                 --\n",
       "│    └─Linear: 2-7                            [64, 128]                 32,896\n",
       "├─TransformerEncoder: 1-4                     [64, 104, 50]             --\n",
       "│    └─ModuleList: 2-8                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [64, 104, 50]             23,378\n",
       "├─Sequential: 1-5                             [64, 100]                 --\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [64, 10, 10]              --\n",
       "│    └─Flatten: 2-10                          [64, 100]                 --\n",
       "├─Sequential: 1-6                             [64, 128]                 --\n",
       "│    └─Linear: 2-11                           [64, 256]                 25,856\n",
       "│    └─BatchNorm1d: 2-12                      [64, 256]                 512\n",
       "│    └─ReLU: 2-13                             [64, 256]                 --\n",
       "│    └─Linear: 2-14                           [64, 128]                 32,896\n",
       "===============================================================================================\n",
       "Total params: 165,284\n",
       "Trainable params: 165,284\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 9.27\n",
       "===============================================================================================\n",
       "Input size (MB): 2.66\n",
       "Forward/backward pass size (MB): 30.26\n",
       "Params size (MB): 0.58\n",
       "Estimated Total Size (MB): 33.50\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfomer_output = tfc.transformer_encoder_f(inputs.permute(0, 2, 1))\n",
    "# linear_output     = tfc.projector_t(transfomer_output.reshape(transfomer_output.shape[0], -1))\n",
    "# print( inputs.permute(0, 2, 1).shape )\n",
    "summary(tfc, ((64, 104, 50), (64, 104, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shapes_dict  = {name: data.shape[1:] for name, data in data_dict.items()}\n",
    "\n",
    "encoder_configs         = Configs(TSlength_aligned=max_seq_len, \n",
    "                                    features_len=inputs.shape[-1], \n",
    "                                    features_len_f=inputs.shape[-1], \n",
    "                                    n_head=1,\n",
    "                                    dim_feedforward=128,\n",
    "                                    num_transformer_layers=1,\n",
    "                                    encoder_layer_dims=128,\n",
    "                                    linear_encoder_dim=256,\n",
    "                                    channel_output_size=10,\n",
    "                                    time_output_size=10\n",
    "                                )\n",
    "\n",
    "tsfm                    = TSFM(input_data_shapes_dict, \n",
    "                                batch_size=128, \n",
    "                                lr=1e-3, \n",
    "                                log=True, \n",
    "                                device='cpu',\n",
    "                                max_train_length=max_seq_len,\n",
    "                                encoder_config=encoder_configs\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abuj/Documents/GitHub/TS-FM/src/TSFM.py:133: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_62dm4livko/croot/pytorch_1675190252673/work/aten/src/ATen/native/IndexingUtils.h:28.)\n",
      "  train_data             = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m config_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdataset_btc\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m128\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     }\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     19\u001b[0m data_set_type \u001b[39m=\u001b[39m ImputationDataset \u001b[39m# Make the ability to infer dataset type based on projection layer \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss          \u001b[39m=\u001b[39m tsfm\u001b[39m.\u001b[39;49mfit(data_dict, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, warmup_projection_layers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, log\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, warmup_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, config_kwargs\u001b[39m=\u001b[39;49mconfig_kwargs, warmup_batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, data_set_type\u001b[39m=\u001b[39;49mdata_set_type)\n",
      "File \u001b[0;32m~/Documents/GitHub/TS-FM/src/TSFM.py:199\u001b[0m, in \u001b[0;36mTSFM.fit\u001b[0;34m(self, train_data_dict, labels, n_epochs, n_iters, verbose, shuffle, warmup_projection_layers, warmup_epochs, log, subset, configs, training_mode, config_kwargs, data_set_type, warmup_batch_size)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m dataset_name, data \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 199\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(data, dataset_name, optimizer, enocder_dataset_type)\n\u001b[1;32m    200\u001b[0m     cum_loss_dict[dataset_name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    202\u001b[0m n_epoch_iters \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/TS-FM/src/TSFM.py:224\u001b[0m, in \u001b[0;36mTSFM.train_step\u001b[0;34m(self, data, dataset_name, optimizer, data_set_type)\u001b[0m\n\u001b[1;32m    220\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTFC\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 224\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step_TFC(data, dataset_name, data_set_type)\n\u001b[1;32m    226\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    227\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/GitHub/TS-FM/src/TSFM.py:266\u001b[0m, in \u001b[0;36mTSFM._train_step_TFC\u001b[0;34m(self, batch, dataset_name, data_set_type)\u001b[0m\n\u001b[1;32m    263\u001b[0m loss_f \u001b[39m=\u001b[39m nt_xent_criterion(h_f, h_f_aug)\n\u001b[1;32m    264\u001b[0m l_TF \u001b[39m=\u001b[39m nt_xent_criterion(z_t, z_f) \u001b[39m# this is the initial version of TF loss\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m l_1, l_2, l_3 \u001b[39m=\u001b[39m nt_xent_criterion(z_t, z_f_aug), nt_xent_criterion(z_t_aug, z_f), nt_xent_criterion(z_t_aug, z_f_aug)\n\u001b[1;32m    267\u001b[0m loss_c \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m l_TF \u001b[39m-\u001b[39m l_1) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m l_TF \u001b[39m-\u001b[39m l_2) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m l_TF \u001b[39m-\u001b[39m l_3)\n\u001b[1;32m    269\u001b[0m lam \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/baseline/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/TS-FM/src/TFC/loss.py:123\u001b[0m, in \u001b[0;36mNTXentLoss_poly.forward\u001b[0;34m(self, zis, zjs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, zis, zjs):\n\u001b[1;32m    121\u001b[0m     representations \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([zjs, zis], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m     similarity_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_function(representations, representations)\n\u001b[1;32m    125\u001b[0m     \u001b[39m# filter out the scores from the positive samples\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     l_pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdiag(similarity_matrix, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/Documents/GitHub/TS-FM/src/TFC/loss.py:117\u001b[0m, in \u001b[0;36mNTXentLoss_poly._cosine_simililarity\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cosine_simililarity\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[1;32m    114\u001b[0m     \u001b[39m# x shape: (N, 1, C)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39m# y shape: (1, 2N, C)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# v shape: (N, 2N)\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cosine_similarity(x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), y\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m v\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/baseline/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/baseline/lib/python3.8/site-packages/torch/nn/modules/distance.py:77\u001b[0m, in \u001b[0;36mCosineSimilarity.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x1: Tensor, x2: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcosine_similarity(x1, x2, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warmup_config_kwargs = {\n",
    "    \"dataset_btc\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"input_channels\": data_dict['dataset_btc'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_btc'].shape[1],\n",
    "    },\n",
    "    \"dataset_eth\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"input_channels\": data_dict['dataset_eth'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_eth'].shape[1],\n",
    "    },\n",
    "    \"dataset_xmr\": {\n",
    "        \"batch_size\": 128,\n",
    "        \"input_channels\": data_dict['dataset_xmr'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_xmr'].shape[1],\n",
    "    }\n",
    "}\n",
    "\n",
    "data_set_type = ImputationDataset # Make the ability to infer dataset type based on projection layer \n",
    "loss          = tsfm.fit(data_dict, n_epochs=10, warmup_projection_layers=False, log=True, verbose=True, shuffle=True, warmup_epochs=10, warmup_config_kwargs=warmup_config_kwargs, warmup_batch_size=128, data_set_type=data_set_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loss import MASE\n",
    "from torchmetrics import MeanAbsolutePercentageError, MeanSquaredLogError\n",
    "\n",
    "loss_mse  = nn.MSELoss()(data_dict['dataset_btc'][0], data_dict['dataset_btc'][1])\n",
    "loss_mape = nn.L1Loss()(data_dict['dataset_btc'][0], data_dict['dataset_btc'][1])\n",
    "# loss_mase = MASE()(data_dict['dataset_btc'][0], data_dict['dataset_btc'][1])\n",
    "loss_mape2 = MeanAbsolutePercentageError()(data_dict['dataset_btc'][3] , data_dict['dataset_btc'][100])\n",
    "loss_msle = MeanSquaredLogError()(data_dict['dataset_btc'][0], data_dict['dataset_btc'][1])\n",
    "\n",
    "\n",
    "print(loss_mse, loss_mape, loss_mape2, loss_msle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'task': 'imputation',\n",
    "    'model': 'transformer',\n",
    "    'num_layers': 2,\n",
    "    'd_model': 32,\n",
    "    'num_heads': 4,\n",
    "    'dim_feedforward': 128,\n",
    "    'dropout': 0.1,\n",
    "    'pos_encoding': 'learnable',\n",
    "    'activation': 'relu',\n",
    "    'normalization_layer': 'layer_norm',\n",
    "    'freeze': False,\n",
    "    'data_window_len': max_seq_len,\n",
    "    'max_seq_len': max_seq_len,\n",
    "}\n",
    "ts_model = model_factory(config, xmr_array.shape[-1])\n",
    "\n",
    "# ts_model(xmr_array[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
