{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchinfo\n",
    "# !pip install lumnisfactors\n",
    "# !pip install matplotlib\n",
    "# !pip install torchmetrics\n",
    "# !conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/baseline/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/baseline/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/envs/baseline/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import grequests\n",
    "\n",
    "from src.mvts_transformer.ts_transformer import TSTransformerEncoder, model_factory\n",
    "from src.utils import create_3d_array, standardize, rolling_mean_diff, generate_univariate_data_labels, generate_data_labels_from_3d_array\n",
    "from src.projection_layers import LSTMMaskedAutoencoderProjection\n",
    "from src.dataset import TSDataset, ImputationDataset\n",
    "from src.dataloader import TSDataLoader\n",
    "from src.TFC.dataloader import TFCDataset\n",
    "from src.encoders import TFC\n",
    "from src.configs import Configs\n",
    "from src.RevIN import RevIN\n",
    "from src.TSFM import TSFM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "!conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumnisfactors import LumnisFactors\n",
    "from KEYS import LUMNIS_API_KEY\n",
    "\n",
    "factorName          = \"price\"\n",
    "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
    "temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "# ob_df_raw           = lumnis.get_historical_data(\"orderbook_snapshot_5\", \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_btc         = rolling_mean_diff(temp_df_btc_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_eth         = rolling_mean_diff(temp_df_eth_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_xmr         = rolling_mean_diff(temp_df_xmr_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "\n",
    "cols                = temp_df_btc.columns #['close', 'volume'] #\n",
    "max_seq_len         = 150\n",
    "\n",
    "btc_array           = create_3d_array(temp_df_btc[cols], temp_df_btc.index, max_seq_len)\n",
    "eth_array           = create_3d_array(temp_df_eth[cols], temp_df_eth.index, max_seq_len)\n",
    "xmr_array           = create_3d_array(temp_df_xmr[cols], temp_df_xmr.index, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56762, 150, 1) (56762, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "univariate_array_eth         = create_3d_array(temp_df_eth_raw[['close']], temp_df_eth_raw.index, max_seq_len)\n",
    "univariate_array_btc         = create_3d_array(temp_df_btc_raw[['close']], temp_df_btc_raw.index, max_seq_len)\n",
    "univariate_array_xmr         = create_3d_array(temp_df_xmr_raw[['close']], temp_df_xmr_raw.index, max_seq_len)\n",
    "\n",
    "uni_data_eth, uni_labels_eth = generate_univariate_data_labels(univariate_array_eth)\n",
    "uni_data_btc, uni_labels_btc = generate_univariate_data_labels(univariate_array_btc)\n",
    "uni_data_xmr, uni_labels_xmr = generate_univariate_data_labels(univariate_array_xmr)\n",
    "\n",
    "uni_data                     = np.concatenate((uni_data_eth, uni_data_btc, uni_data_xmr), axis=0)\n",
    "uni_labels                   = np.concatenate((uni_labels_eth, uni_labels_btc, uni_labels_xmr), axis=0)\n",
    "\n",
    "print(uni_data.shape, uni_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btc, labels_btc = generate_data_labels_from_3d_array(btc_array)\n",
    "data_eth, labels_eth = generate_data_labels_from_3d_array(eth_array)\n",
    "data_xmr, labels_xmr = generate_data_labels_from_3d_array(xmr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data as a dictionary\n",
    "data_dict = {\n",
    "    'univariate': {\"data\": uni_data, \"labels\": uni_labels},\n",
    "    'dataset_btc': {'data': data_btc, 'labels': labels_btc},\n",
    "    'dataset_eth': {'data': data_eth, 'labels': labels_eth},\n",
    "    'dataset_xmr': {'data': data_xmr, 'labels': labels_xmr},#xmr_array,\n",
    "    \n",
    "}\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "for key in data_dict.keys():\n",
    "    if type(data_dict[key]) == dict: \n",
    "        data_dict[key]['data'] = torch.from_numpy( data_dict[key]['data'] ).to(torch.float32)\n",
    "        data_dict[key]['labels'] = torch.from_numpy( data_dict[key]['labels'] ).to(torch.float32)\n",
    "    else:\n",
    "        data_dict[key] = torch.from_numpy( data_dict[key] ).to(torch.float32)\n",
    "           \n",
    "# Create instances of TSDataset for each dataset\n",
    "datasets = { name: (TSDataset(data['data'], data['labels'], max_len=max_seq_len, shuffle=True) if type(data)==dict\n",
    "          else ImputationDataset(data, masking_ratio=0.25)) for name, data in data_dict.items() }\n",
    "\n",
    "# Create an instance of the custom data loader\n",
    "ts_data_loader = TSDataLoader(datasets, batch_size=512, max_len=max_seq_len, collate_fn='unsuperv', shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shapes_dict  = {name: data['data'].shape[1:] if type(data)==dict else data.shape[1:] for name, data in data_dict.items()}\n",
    "\n",
    "\n",
    "DEVICE                  = 'cuda'\n",
    "MAX_SEQ_LENGTH          = max_seq_len\n",
    "ENCODER_LAYER_DIMS      = 64\n",
    "PROJECTION_DIMS         = 128\n",
    "\n",
    "\n",
    "encoder_configs         = Configs(TSlength_aligned=max_seq_len, \n",
    "                                    features_len=PROJECTION_DIMS, \n",
    "                                    features_len_f=PROJECTION_DIMS, \n",
    "                                    encoder_layer_dims=ENCODER_LAYER_DIMS,\n",
    "                                    dim_feedforward=128,\n",
    "                                    linear_encoder_dim=256,\n",
    "                                    channel_output_size=10,\n",
    "                                    time_output_size=10,\n",
    "                                    d_model=128,\n",
    "                                    num_transformer_layers=1,\n",
    "                                    n_head=1,\n",
    "                                    pos_encoding='learnable',\n",
    "                                    transformer_activation='gelu',\n",
    "                                    transformer_normalization_layer='BatchNorm',\n",
    "                                    freeze=False,\n",
    "                                )\n",
    "\n",
    "tsfm                    = TSFM(input_data_shapes_dict, \n",
    "                                device=DEVICE,\n",
    "                                max_seq_length=max_seq_len,\n",
    "                                encoder_config=encoder_configs,\n",
    "                                projection_layer_dims=PROJECTION_DIMS,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_config_kwargs = {\n",
    "    \"dataset_btc\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_btc']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_btc']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_eth\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_eth']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_eth']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_xmr\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_xmr']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_xmr']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }, \n",
    "    \"univariate\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['univariate']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['univariate']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# TODO: Add learning rate to warmup config kwargs\n",
    "\n",
    "N_EPOCHS                 = 10\n",
    "WARMUP_EPOCHS            = 10\n",
    "WARMUP_BATCH_SIZE        = 512\n",
    "WARMUP_PROJECTION_LAYERS = True\n",
    "BATCH_SIZE               = 512\n",
    "LR                       = 1e-2\n",
    "LOG                      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 221054\n",
      "Warming up with 111 batches of size 512. Dataset name univariate.\n",
      "Epoch: 0, Loss: 786.5895787144566\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss          \u001b[39m=\u001b[39m tsfm\u001b[39m.\u001b[39;49mfit(data_dict, n_epochs\u001b[39m=\u001b[39;49mN_EPOCHS, warmup_projection_layers\u001b[39m=\u001b[39;49mWARMUP_PROJECTION_LAYERS, \n\u001b[1;32m      2\u001b[0m                          log\u001b[39m=\u001b[39;49mLOG, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, warmup_epochs\u001b[39m=\u001b[39;49mWARMUP_EPOCHS, \n\u001b[1;32m      3\u001b[0m                          warmup_config_kwargs\u001b[39m=\u001b[39;49mwarmup_config_kwargs, warmup_batch_size\u001b[39m=\u001b[39;49mWARMUP_BATCH_SIZE,\n\u001b[1;32m      4\u001b[0m                          batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE, lr\u001b[39m=\u001b[39;49mLR, device\u001b[39m=\u001b[39;49mDEVICE, max_seq_length\u001b[39m=\u001b[39;49mMAX_SEQ_LENGTH, \n\u001b[1;32m      5\u001b[0m                         )\n",
      "File \u001b[0;32m~/TS-FM/src/TSFM.py:128\u001b[0m, in \u001b[0;36mTSFM.fit\u001b[0;34m(self, train_data_dict, labels, lr, n_epochs, batch_size, n_iters, verbose, shuffle, warmup_projection_layers, warmup_epochs, log, subset, configs, training_mode, warmup_config_kwargs, warmup_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal number of data points: \u001b[39m\u001b[39m{\u001b[39;00mtotal_number_of_data_points\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Warmup the projection layers\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m datasets, optimizer_list, encoder_dataset_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarmup(train_data_dict, warmup_projection_layers\u001b[39m=\u001b[39;49mwarmup_projection_layers, \n\u001b[1;32m    129\u001b[0m                                                              warmup_epochs\u001b[39m=\u001b[39;49mwarmup_epochs, shuffle\u001b[39m=\u001b[39;49mshuffle, \n\u001b[1;32m    130\u001b[0m                                                              warmup_config_kwargs\u001b[39m=\u001b[39;49mwarmup_config_kwargs, warmup_batch_size\u001b[39m=\u001b[39;49mwarmup_batch_size, \n\u001b[1;32m    131\u001b[0m                                                              lr\u001b[39m=\u001b[39;49mlr, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    132\u001b[0m                                                              )\n\u001b[1;32m    134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Disable revIN for univariate forcasting\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39munivariate\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_layers\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/TS-FM/src/TSFM.py:221\u001b[0m, in \u001b[0;36mTSFM.warmup\u001b[0;34m(self, train_data_dict, labels, lr, n_epochs, n_iters, verbose, shuffle, warmup_projection_layers, warmup_epochs, log, subset, configs, training_mode, warmup_config_kwargs, warmup_batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     pl_kwargs \u001b[39m=\u001b[39m warmup_config[\u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m warmup_config \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    220\u001b[0m     pl_lr    \u001b[39m=\u001b[39m lr \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m warmup_config \u001b[39melse\u001b[39;00m warmup_config[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 221\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_projection_layers[dataset_name]\u001b[39m.\u001b[39;49mwarmup(dataset, n_epochs\u001b[39m=\u001b[39;49mn_epochs, batch_size\u001b[39m=\u001b[39;49mbatch_s, learning_rate\u001b[39m=\u001b[39;49mpl_lr, \n\u001b[1;32m    222\u001b[0m                                                  log\u001b[39m=\u001b[39;49mlog, data_set_type\u001b[39m=\u001b[39;49mwarmup_config[\u001b[39m'\u001b[39;49m\u001b[39mdata_set_type\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m    223\u001b[0m                                                  collate_fn\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39munsuperv\u001b[39;49m\u001b[39m'\u001b[39;49m, max_len\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length, \n\u001b[1;32m    224\u001b[0m                                                  dataset_name\u001b[39m=\u001b[39;49mdataset_name,\n\u001b[1;32m    225\u001b[0m                                                  \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpl_kwargs\n\u001b[1;32m    226\u001b[0m                                                  ) \n\u001b[1;32m    228\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initialize datasets\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m encoder_dataset_type \u001b[39m=\u001b[39m TSDataset\n",
      "File \u001b[0;32m~/TS-FM/src/projection_layers.py:89\u001b[0m, in \u001b[0;36mBaseProjectionLayer.warmup\u001b[0;34m(self, dataset, max_len, n_epochs, batch_size, learning_rate, log, data_set_type, collate_fn, scheduler_step_size, scheduler_gamma, verbose, dataset_name, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[1;32m     88\u001b[0m     cum_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     90\u001b[0m         \u001b[39m# Get the inputs\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         inputs, targets, target_masks, padding_masks, data_time_feat, label_time_feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_inputs(data, data_set_type)\n\u001b[1;32m     93\u001b[0m         \u001b[39m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/baseline/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/baseline/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/baseline/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/baseline/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/TS-FM/src/dataset.py:60\u001b[0m, in \u001b[0;36mTSDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     58\u001b[0m shuffled_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[index]\n\u001b[1;32m     59\u001b[0m length \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[shuffled_index]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n\u001b[0;32m---> 60\u001b[0m padding_masks \u001b[39m=\u001b[39m padding_mask(torch\u001b[39m.\u001b[39;49mtensor(length, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint16), max_len\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_len) \n\u001b[1;32m     61\u001b[0m padding_masks \u001b[39m=\u001b[39m padding_masks\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[shuffled_index] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_passed \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[shuffled_index] \n",
      "File \u001b[0;32m~/TS-FM/src/dataset.py:422\u001b[0m, in \u001b[0;36mpadding_mask\u001b[0;34m(lengths, max_len)\u001b[0m\n\u001b[1;32m    420\u001b[0m batch_size \u001b[39m=\u001b[39m lengths\u001b[39m.\u001b[39mnumel()\n\u001b[1;32m    421\u001b[0m max_len \u001b[39m=\u001b[39m max_len \u001b[39mor\u001b[39;00m lengths\u001b[39m.\u001b[39mmax_val()  \u001b[39m# trick works because of overloading of 'or' operator for non-boolean types\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39;49marange(\u001b[39m0\u001b[39;49m, max_len, device\u001b[39m=\u001b[39;49mlengths\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    423\u001b[0m         \u001b[39m.\u001b[39;49mtype_as(lengths)\n\u001b[1;32m    424\u001b[0m         \u001b[39m.\u001b[39;49mrepeat(batch_size, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    425\u001b[0m         \u001b[39m.\u001b[39mlt(lengths\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss          = tsfm.fit(data_dict, n_epochs=N_EPOCHS, warmup_projection_layers=WARMUP_PROJECTION_LAYERS, \n",
    "                         log=LOG, verbose=True, shuffle=True, warmup_epochs=WARMUP_EPOCHS, \n",
    "                         warmup_config_kwargs=warmup_config_kwargs, warmup_batch_size=WARMUP_BATCH_SIZE,\n",
    "                         batch_size=BATCH_SIZE, lr=LR, device=DEVICE, max_seq_length=MAX_SEQ_LENGTH, \n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dataset_xmr'\n",
    "inputs       = data_dict[dataset_name]['data'][:300]\n",
    "repr = tsfm.encode(inputs, 128, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
