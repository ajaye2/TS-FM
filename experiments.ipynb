{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchsummary\n",
    "# !pip install torchinfo\n",
    "# !pip install lumnisfactors\n",
    "# !pip install matplotlib\n",
    "# !pip install torchmetrics\n",
    "# !conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/baseline/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/baseline/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/conda/envs/baseline/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.1\n",
      "  latest version: 23.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import grequests\n",
    "\n",
    "from src.mvts_transformer.ts_transformer import TSTransformerEncoder, model_factory\n",
    "from src.utils import create_3d_array, standardize, rolling_mean_diff, generate_univariate_data_labels, generate_data_labels_from_3d_array\n",
    "from src.projection_layers import LSTMMaskedAutoencoderProjection\n",
    "from src.dataset import TSDataset, ImputationDataset\n",
    "from src.dataloader import TSDataLoader\n",
    "from src.TFC.dataloader import TFCDataset\n",
    "from src.encoders import TFC\n",
    "from src.configs import Configs\n",
    "from src.RevIN import RevIN\n",
    "from src.TSFM import TSFM\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft as fft\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(5000)\n",
    "\n",
    "!conda install cudnn=8.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumnisfactors import LumnisFactors\n",
    "from KEYS import LUMNIS_API_KEY\n",
    "\n",
    "factorName          = \"price\"\n",
    "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
    "temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
    "# ob_df_raw           = lumnis.get_historical_data(\"orderbook_snapshot_5\", \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_btc         = rolling_mean_diff(temp_df_btc_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_eth         = rolling_mean_diff(temp_df_eth_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "temp_df_xmr         = rolling_mean_diff(temp_df_xmr_raw, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], type='standard')\n",
    "\n",
    "cols                = temp_df_btc.columns #['close', 'volume'] #\n",
    "max_seq_len         = 150\n",
    "\n",
    "btc_array           = create_3d_array(temp_df_btc[cols], temp_df_btc.index, max_seq_len)\n",
    "eth_array           = create_3d_array(temp_df_eth[cols], temp_df_eth.index, max_seq_len)\n",
    "xmr_array           = create_3d_array(temp_df_xmr[cols], temp_df_xmr.index, max_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56762, 150, 1) (56762, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "univariate_array_eth         = create_3d_array(temp_df_eth_raw[['close']], temp_df_eth_raw.index, max_seq_len)\n",
    "univariate_array_btc         = create_3d_array(temp_df_btc_raw[['close']], temp_df_btc_raw.index, max_seq_len)\n",
    "univariate_array_xmr         = create_3d_array(temp_df_xmr_raw[['close']], temp_df_xmr_raw.index, max_seq_len)\n",
    "\n",
    "uni_data_eth, uni_labels_eth = generate_univariate_data_labels(univariate_array_eth)\n",
    "uni_data_btc, uni_labels_btc = generate_univariate_data_labels(univariate_array_btc)\n",
    "uni_data_xmr, uni_labels_xmr = generate_univariate_data_labels(univariate_array_xmr)\n",
    "\n",
    "uni_data                     = np.concatenate((uni_data_eth, uni_data_btc, uni_data_xmr), axis=0)\n",
    "uni_labels                   = np.concatenate((uni_labels_eth, uni_labels_btc, uni_labels_xmr), axis=0)\n",
    "\n",
    "print(uni_data.shape, uni_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btc, labels_btc = generate_data_labels_from_3d_array(btc_array)\n",
    "data_eth, labels_eth = generate_data_labels_from_3d_array(eth_array)\n",
    "data_xmr, labels_xmr = generate_data_labels_from_3d_array(xmr_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data as a dictionary\n",
    "data_dict = {\n",
    "    'univariate': {\"data\": uni_data, \"labels\": uni_labels},\n",
    "    'dataset_btc': {'data': data_btc, 'labels': labels_btc},\n",
    "    'dataset_eth': {'data': data_eth, 'labels': labels_eth},\n",
    "    'dataset_xmr': {'data': data_xmr, 'labels': labels_xmr},#xmr_array,\n",
    "    \n",
    "}\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "for key in data_dict.keys():\n",
    "    if type(data_dict[key]) == dict: \n",
    "        data_dict[key]['data'] = torch.from_numpy( data_dict[key]['data'] ).to(torch.float32)\n",
    "        data_dict[key]['labels'] = torch.from_numpy( data_dict[key]['labels'] ).to(torch.float32)\n",
    "    else:\n",
    "        data_dict[key] = torch.from_numpy( data_dict[key] ).to(torch.float32)\n",
    "           \n",
    "# Create instances of TSDataset for each dataset\n",
    "datasets = { name: (TSDataset(data['data'], data['labels'], max_len=max_seq_len, shuffle=True) if type(data)==dict\n",
    "          else ImputationDataset(data, masking_ratio=0.25)) for name, data in data_dict.items() }\n",
    "\n",
    "# Create an instance of the custom data loader\n",
    "ts_data_loader = TSDataLoader(datasets, batch_size=512, max_len=max_seq_len, collate_fn='unsuperv', shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_shapes_dict  = {name: data['data'].shape[1:] if type(data)==dict else data.shape[1:] for name, data in data_dict.items()}\n",
    "\n",
    "\n",
    "DEVICE                  = 'cuda'\n",
    "MAX_SEQ_LENGTH          = max_seq_len\n",
    "ENCODER_LAYER_DIMS      = 64\n",
    "PROJECTION_DIMS         = 128\n",
    "\n",
    "\n",
    "encoder_configs         = Configs(TSlength_aligned=max_seq_len, \n",
    "                                    features_len=PROJECTION_DIMS, \n",
    "                                    features_len_f=PROJECTION_DIMS, \n",
    "                                    encoder_layer_dims=ENCODER_LAYER_DIMS,\n",
    "                                    dim_feedforward=128,\n",
    "                                    linear_encoder_dim=256,\n",
    "                                    channel_output_size=10,\n",
    "                                    time_output_size=10,\n",
    "                                    d_model=128,\n",
    "                                    num_transformer_layers=1,\n",
    "                                    n_head=1,\n",
    "                                    pos_encoding='learnable',\n",
    "                                    transformer_activation='gelu',\n",
    "                                    transformer_normalization_layer='BatchNorm',\n",
    "                                    freeze=False,\n",
    "                                )\n",
    "\n",
    "tsfm                    = TSFM(input_data_shapes_dict, \n",
    "                                device=DEVICE,\n",
    "                                max_seq_length=max_seq_len,\n",
    "                                encoder_config=encoder_configs,\n",
    "                                projection_layer_dims=PROJECTION_DIMS,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_config_kwargs = {\n",
    "    \"dataset_btc\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_btc']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_btc']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_eth\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_eth']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_eth']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    },\n",
    "    \"dataset_xmr\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['dataset_xmr']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['dataset_xmr']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }, \n",
    "    \"univariate\": {\n",
    "        \"batch_size\": 512,\n",
    "        \"input_channels\": data_dict['univariate']['data'].shape[-1],\n",
    "        \"timesteps\": data_dict['univariate']['data'].shape[1],\n",
    "        \"data_set_type\": TSDataset,\n",
    "        \"num_epochs\": 30,\n",
    "        \"lr\": 1e-2,\n",
    "        \"kwargs\": {\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# TODO: Add learning rate to warmup config kwargs\n",
    "\n",
    "N_EPOCHS                 = 10\n",
    "WARMUP_EPOCHS            = 10\n",
    "WARMUP_BATCH_SIZE        = 512\n",
    "WARMUP_PROJECTION_LAYERS = True\n",
    "BATCH_SIZE               = 512\n",
    "LR                       = 1e-2\n",
    "LOG                      = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points: 221054\n",
      "Warming up with 111 batches of size 512. Dataset name univariate.\n",
      "Epoch: 0, Loss: 767.3070788684192\n",
      "Epoch: 1, Loss: 760.2686118735924\n",
      "Epoch: 2, Loss: 762.9503613721143\n",
      "Epoch: 3, Loss: 755.1848815368103\n",
      "Epoch: 4, Loss: 751.0378335488809\n",
      "Epoch: 5, Loss: 748.7066413948128\n",
      "Epoch: 6, Loss: 752.448643589879\n",
      "Epoch: 7, Loss: 721.6355090442005\n",
      "Epoch: 8, Loss: 715.1178770323057\n",
      "Epoch: 9, Loss: 739.0325751777168\n",
      "Warming up with 36 batches of size 512. Dataset name dataset_btc.\n",
      "Epoch: 0, Loss: 0.8723262531889809\n",
      "Epoch: 1, Loss: 0.8193830135795805\n",
      "Epoch: 2, Loss: 0.7438982559574975\n",
      "Epoch: 3, Loss: 0.6627913614114126\n",
      "Epoch: 4, Loss: 0.5915808379650116\n",
      "Epoch: 5, Loss: 0.5376250379615359\n",
      "Epoch: 6, Loss: 0.497392223113113\n",
      "Epoch: 7, Loss: 0.4657358295387692\n",
      "Epoch: 8, Loss: 0.43463804655604893\n",
      "Epoch: 9, Loss: 0.41313982341024613\n",
      "Warming up with 36 batches of size 512. Dataset name dataset_eth.\n",
      "Epoch: 0, Loss: 0.8413881460825602\n",
      "Epoch: 1, Loss: 0.7769616660144594\n",
      "Epoch: 2, Loss: 0.7109624611006843\n",
      "Epoch: 3, Loss: 0.6313604894611571\n",
      "Epoch: 4, Loss: 0.570143289036221\n",
      "Epoch: 5, Loss: 0.5139945223927498\n",
      "Epoch: 6, Loss: 0.4660848528146744\n",
      "Epoch: 7, Loss: 0.4331770795914862\n",
      "Epoch: 8, Loss: 0.40650615841150284\n",
      "Epoch: 9, Loss: 0.39446976284186047\n",
      "Warming up with 35 batches of size 512. Dataset name dataset_xmr.\n",
      "Epoch: 0, Loss: 0.850504401751927\n",
      "Epoch: 1, Loss: 0.8158660939761571\n",
      "Epoch: 2, Loss: 0.7636373502867563\n",
      "Epoch: 3, Loss: 0.698099958896637\n",
      "Epoch: 4, Loss: 0.6378397362572806\n",
      "Epoch: 5, Loss: 0.5863143937928336\n",
      "Epoch: 6, Loss: 0.544667603288378\n",
      "Epoch: 7, Loss: 0.5056936136313848\n",
      "Epoch: 8, Loss: 0.47428896852902\n",
      "Epoch: 9, Loss: 0.4516412786075047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss                     = tsfm.fit(data_dict, n_epochs=N_EPOCHS, warmup_projection_layers=WARMUP_PROJECTION_LAYERS, \n",
    "                                    log=LOG, verbose=True, shuffle=True, warmup_epochs=WARMUP_EPOCHS, \n",
    "                                    warmup_config_kwargs=warmup_config_kwargs, warmup_batch_size=WARMUP_BATCH_SIZE,\n",
    "                                    batch_size=BATCH_SIZE, lr=LR, device=DEVICE, max_seq_length=MAX_SEQ_LENGTH, \n",
    "                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'dataset_xmr'\n",
    "inputs       = data_dict[dataset_name]['data'][:300]\n",
    "repr         = tsfm.encode(inputs, 128, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 128])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
