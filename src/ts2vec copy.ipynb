{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bgG5AE6THnZG"
      },
      "outputs": [],
      "source": [
        "# %pip show pip\n",
        "# %pip install -U scikit-learn\n",
        "# python3 -m pip3 install jupyter notebook -U\n",
        "\n",
        "# !pip install torch==1.8.1\n",
        "# !pip install scipy==1.6.1\n",
        "# !pip install numpy==1.19.2\n",
        "# !pip install pandas==1.0.1\n",
        "# !pip install scikit_learn==0.24.2\n",
        "# !pip install statsmodels==0.12.2\n",
        "# !pip install Bottleneck==1.3.2\n",
        "# !pip install lumnisfactors\n",
        "\n",
        "# !pip install torch==1.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/azureuser/TS-FM/enter/envs/ts2vec/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 1.9.0+cu111\n",
            "CUDA available: True\n",
            "cuDNN version: 8005\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "    !unset LD_LIBRARY_PATH\n",
        "    print(\"Torch version:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "except:\n",
        "    !unset LD_LIBRARY_PATH\n",
        "    print(\"Torch version:\", torch.__version__)\n",
        "    print(\"CUDA available:\", torch.cuda.is_available())\n",
        "    print(\"cuDNN version:\", torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZekWOGoDHpgE",
        "outputId": "ecb16a4a-bb92-4338-e46a-328fbf201b7d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ReRvaPuHnZJ",
        "outputId": "69ad5772-fc12-488f-fe4c-4b57b3045b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# from train_utils import get_data, train_model, save_checkpoint_callback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import linear_model\n",
        "# from utils import standardize\n",
        "from datetime import datetime\n",
        "from models.losses import *\n",
        "from ts2vec import TS2Vec\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from os import walk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datautils\n",
        "import torch\n",
        "import json\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import create_3d_array, standardize, rolling_mean_diff, generate_univariate_data_labels, generate_data_labels_from_3d_array, get_train_val_test_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lumnisfactors import LumnisFactors\n",
        "from KEYS import LUMNIS_API_KEY\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "# [\"ADAUSD\", \"BTCUSD\", \"DASHUSD\", \"DOGEUSD\", \"DOTUSD\", \"ETHUSD\", \"LTCUSD\", \"NEOUSD\", \"XMRUSD\", \"XRPUSD\", \"XBTUSD\", \"SOLUSD\", \"BNBUSD\", \"AVAXUSD\" \"MATICUSD‚Äù] \n",
        "\n",
        "factorName          = \"price\"\n",
        "lumnis              = LumnisFactors(LUMNIS_API_KEY)\n",
        "path_to_data = \"/home/ec2-user/TS-FM/src/data/\"\n",
        "path_to_data = \"/home/azureuser/TS-FM/src/data/\"\n",
        "\n",
        "btc_file = Path(path_to_data + \"btc.csv\")\n",
        "eth_file = Path(path_to_data + \"eth.csv\")\n",
        "xmr_file = Path(path_to_data + \"xmr.csv\")\n",
        "ada_file = Path(path_to_data + \"ada.csv\")\n",
        "doge_file = Path(path_to_data + \"doge.csv\")\n",
        "bnb_file = Path(path_to_data + \"bnb.csv\")\n",
        "dot_file = Path(path_to_data + \"dot.csv\")\n",
        "ltc_file = Path(path_to_data + \"ltc.csv\")\n",
        "dash_file = Path(path_to_data + \"dash.csv\")\n",
        "neo_file = Path(path_to_data + \"neo.csv\")\n",
        "xrp_file = Path(path_to_data + \"xrp.csv\")\n",
        "sol_file = Path(path_to_data + \"sol.csv\")\n",
        "avax_file = Path(path_to_data + \"avax.csv\")\n",
        "matic_file = Path(path_to_data + \"matic.csv\")\n",
        "\n",
        "if btc_file.is_file():\n",
        "    temp_df_btc_raw     = pd.read_csv(path_to_data + \"btc.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_btc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"btcusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
        "    temp_df_btc_raw.to_csv(path_to_data + \"btc.csv\")\n",
        "\n",
        "if eth_file.is_file():\n",
        "    temp_df_eth_raw     = pd.read_csv(path_to_data + \"eth.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_eth_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ethusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
        "    temp_df_eth_raw.to_csv(path_to_data + \"eth.csv\")\n",
        "\n",
        "if xmr_file.is_file():\n",
        "    temp_df_xmr_raw     = pd.read_csv(path_to_data + \"xmr.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_xmr_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xmrusdt\",  \"hour\", \"2021-01-23\", \"2023-04-16\")\n",
        "    temp_df_xmr_raw.to_csv(path_to_data + \"xmr.csv\")\n",
        "\n",
        "if ada_file.is_file():\n",
        "    temp_df_ada_raw     = pd.read_csv(path_to_data + \"ada.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_ada_raw     = lumnis.get_historical_data(factorName, \"binance\", \"adausdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_ada_raw.to_csv(path_to_data + \"ada.csv\")\n",
        "\n",
        "if doge_file.is_file():\n",
        "    temp_df_doge_raw     = pd.read_csv(path_to_data + \"doge.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_doge_raw     = lumnis.get_historical_data(factorName, \"binance\", \"dogeusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_doge_raw.to_csv(path_to_data + \"doge.csv\")\n",
        "\n",
        "if bnb_file.is_file():\n",
        "    temp_df_bnb_raw     = pd.read_csv(path_to_data + \"bnb.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_bnb_raw     = lumnis.get_historical_data(factorName, \"binance\", \"bnbusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_bnb_raw.to_csv(path_to_data + \"bnb.csv\")\n",
        "\n",
        "if dot_file.is_file():\n",
        "    temp_df_dot_raw     = pd.read_csv(path_to_data + \"dot.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_dot_raw     = lumnis.get_historical_data(factorName, \"binance\", \"dotusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_dot_raw.to_csv(path_to_data + \"dot.csv\")\n",
        "\n",
        "if ltc_file.is_file():\n",
        "    temp_df_ltc_raw     = pd.read_csv(path_to_data + \"ltc.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_ltc_raw     = lumnis.get_historical_data(factorName, \"binance\", \"ltcusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_ltc_raw.to_csv(path_to_data + \"ltc.csv\")\n",
        "\n",
        "if dash_file.is_file():\n",
        "    temp_df_dash_raw     = pd.read_csv(path_to_data + \"dash.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_dash_raw     = lumnis.get_historical_data(factorName, \"binance\", \"dashusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_dash_raw.to_csv(path_to_data + \"dash.csv\")\n",
        "\n",
        "if neo_file.is_file():\n",
        "    temp_df_neo_raw     = pd.read_csv(path_to_data + \"neo.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_neo_raw     = lumnis.get_historical_data(factorName, \"binance\", \"neousdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_neo_raw.to_csv(path_to_data + \"neo.csv\")\n",
        "\n",
        "if xrp_file.is_file():\n",
        "    temp_df_xrp_raw     = pd.read_csv(path_to_data + \"xrp.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_xrp_raw     = lumnis.get_historical_data(factorName, \"binance\", \"xrpusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_xrp_raw.to_csv(path_to_data + \"xrp.csv\")\n",
        "\n",
        "if sol_file.is_file():\n",
        "    temp_df_sol_raw     = pd.read_csv(path_to_data + \"sol.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_sol_raw     = lumnis.get_historical_data(factorName, \"binance\", \"solusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_sol_raw.to_csv(path_to_data + \"sol.csv\")\n",
        "\n",
        "if avax_file.is_file():\n",
        "    temp_df_avax_raw     = pd.read_csv(path_to_data + \"avax.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_avax_raw     = lumnis.get_historical_data(factorName, \"binance\", \"avaxusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_avax_raw.to_csv(path_to_data + \"avax.csv\")\n",
        "\n",
        "if matic_file.is_file():\n",
        "    temp_df_matic_raw     = pd.read_csv(path_to_data + \"matic.csv\").set_index(\"Unnamed: 0\")\n",
        "else:\n",
        "    temp_df_matic_raw     = lumnis.get_historical_data(factorName, \"binance\", \"maticusdt\",  \"hour\", \"2019-04-01\", \"2023-05-01\")\n",
        "    temp_df_matic_raw.to_csv(path_to_data + \"matic.csv\")\n",
        "\n",
        "# TODO: Add resample and fillna with ffill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(26968, 150, 104) (3371, 150, 104) (3372, 150, 104)\n",
            "(26548, 150, 104) (3318, 150, 104) (3319, 150, 104)\n",
            "(16909, 150, 104) (2113, 150, 104) (2115, 150, 104)\n",
            "(22810, 150, 104) (2851, 150, 104) (2852, 150, 104)\n"
          ]
        }
      ],
      "source": [
        "all_data_dict_df = {\n",
        "    \"btc\"   : temp_df_btc_raw,\n",
        "    \"eth\"   : temp_df_eth_raw,\n",
        "    # \"xmr\"   : temp_df_xmr_raw,\n",
        "    # \"ada\"   : temp_df_ada_raw,\n",
        "    # \"bnb\"   : temp_df_bnb_raw,\n",
        "\n",
        "    # \"doge\"  : temp_df_doge_raw,\n",
        "    # \"dot\"   : temp_df_dot_raw,\n",
        "    # \"ltc\"   : temp_df_ltc_raw,\n",
        "    # \"dash\"  : temp_df_dash_raw,\n",
        "    # \"neo\"   : temp_df_neo_raw,\n",
        "    # \"xrp\"   : temp_df_xrp_raw,\n",
        "    # \"sol\"   : temp_df_sol_raw,\n",
        "    \"avax\"  : temp_df_avax_raw,\n",
        "    \"matic\" : temp_df_matic_raw\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "TEST_SYMBOLS = [\"avax\", \"matic\"]\n",
        "# TEST = True\n",
        "# if TEST:\n",
        "#     all_data_dict_df[\"avax\"] = temp_df_avax_raw # DATA THE MODEL HAS NOT SEEN\n",
        "#     all_data_dict_df[\"matic\"] = temp_df_matic_raw # DATA THE MODEL HAS NOT SEEN\n",
        "\n",
        "\n",
        "all_data_rolling_df = {}\n",
        "type_rol = 'standard'\n",
        "\n",
        "for key, value in all_data_dict_df.items():\n",
        "    all_data_rolling_df[key] = rolling_mean_diff(value, [ 5, 25, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000 ], type=type_rol)\n",
        "\n",
        "data_dict_array  = {}\n",
        "max_seq_len     = 150\n",
        "\n",
        "for key, value in all_data_rolling_df.items():\n",
        "    array, array_idxs = create_3d_array(value, value.index, max_seq_len)\n",
        "    data_dict_array[key + \"_data\"] = array\n",
        "    data_dict_array[key + \"_idxs\"] = array_idxs\n",
        "\n",
        "    train_array, val_array, test_array = get_train_val_test_array(array, 0.8, 0.1, 0.1)\n",
        "    data_dict_array[key + \"_train_data\"] = train_array\n",
        "    data_dict_array[key + \"_val_data\"] = val_array\n",
        "    data_dict_array[key + \"_test_data\"] = test_array\n",
        "\n",
        "    train_idxs, val_idxs, test_idxs = get_train_val_test_array(array_idxs, 0.8, 0.1, 0.1)\n",
        "    data_dict_array[key + \"_train_idxs\"] = train_idxs\n",
        "    data_dict_array[key + \"_val_idxs\"] = val_idxs\n",
        "    data_dict_array[key + \"_test_idxs\"] = test_idxs\n",
        "\n",
        "    print(train_array.shape, val_array.shape, test_array.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_loss(data_dict, asset, n_epochs=35):\n",
        "\n",
        "    train_data = data_dict[asset + \"_train_data\"]\n",
        "\n",
        "    model = TS2Vec(\n",
        "        input_dims=train_data.shape[-1],\n",
        "        device=0,\n",
        "        output_dims=64,\n",
        "        batch_size=512\n",
        "    )\n",
        "\n",
        "    loss_log = model.fit(\n",
        "        train_data,\n",
        "        verbose=True,\n",
        "        n_epochs=n_epochs\n",
        "    )\n",
        "\n",
        "    return model, loss_log\n",
        "\n",
        "    \n",
        "def get_all_data_for_experiment(data_dict_array, df_raw, asset, model, pred_len=1):\n",
        "\n",
        "\n",
        "    train_data = data_dict_array[asset + \"_train_data\"]\n",
        "    val_data = data_dict_array[asset + \"_val_data\"]\n",
        "    test_data = data_dict_array[asset + \"_test_data\"]\n",
        "\n",
        "    train_idxs = data_dict_array[asset + \"_train_idxs\"]\n",
        "    val_idxs = data_dict_array[asset + \"_val_idxs\"]\n",
        "    test_idxs = data_dict_array[asset + \"_test_idxs\"]\n",
        "\n",
        "    repr_train_data = model.encode(train_data, encoding_window='full_series')\n",
        "    repr_val_data = model.encode(val_data, encoding_window='full_series')\n",
        "    repr_test_data = model.encode(test_data, encoding_window='full_series')\n",
        "\n",
        "    repr_train_df = pd.DataFrame(repr_train_data, index=train_idxs)\n",
        "    repr_val_df = pd.DataFrame(repr_val_data, index=val_idxs)\n",
        "    repr_test_df = pd.DataFrame(repr_test_data, index=test_idxs)\n",
        "\n",
        "\n",
        "    close_train = df_raw.loc[train_idxs]['close'] \n",
        "    close_val = df_raw.loc[val_idxs]['close']\n",
        "    close_test = df_raw.loc[test_idxs]['close']\n",
        "\n",
        "    price_dir_train = np.sign((close_train.shift(-pred_len) - close_train) / close_train).fillna(0)\n",
        "    price_dir_val = np.sign((close_val.shift(-pred_len) - close_val) / close_val).fillna(0)\n",
        "    price_dir_test = np.sign((close_test.shift(-pred_len) - close_test) / close_test).fillna(0)\n",
        "\n",
        "    price_dir_train[price_dir_train == -1] = 0\n",
        "    price_dir_val[price_dir_val == -1] = 0\n",
        "    price_dir_test[price_dir_test == -1] = 0\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"train_data\" : train_data,\n",
        "        \"val_data\" : val_data,\n",
        "        \"test_data\" : test_data,\n",
        "        \"train_idxs\" : train_idxs,\n",
        "        \"val_idxs\" : val_idxs,\n",
        "        \"test_idxs\" : test_idxs,\n",
        "        \"repr_train_data\" : repr_train_data,\n",
        "        \"repr_val_data\" : repr_val_data,\n",
        "        \"repr_test_data\" : repr_test_data,\n",
        "        \"repr_train_df\" : repr_train_df,\n",
        "        \"repr_val_df\" : repr_val_df,\n",
        "        \"repr_test_df\" : repr_test_df,\n",
        "        \"close_train\" : close_train,\n",
        "        \"close_val\" : close_val,\n",
        "        \"close_test\" : close_test,\n",
        "        \"price_dir_train\" : price_dir_train,\n",
        "        \"price_dir_val\" : price_dir_val,\n",
        "        \"price_dir_test\" : price_dir_test\n",
        "        \n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def get_train_and_get_f1_acc(data_dict_exp, pred_len=1):\n",
        "    log_reg = Pipeline([\n",
        "            # ('feature_selection', SelectFromModel(RandomForestClassifier(max_depth=2))),\n",
        "            # ('clf', RandomForestClassifier(max_depth=1))\n",
        "            ('clf', LogisticRegression(max_iter=1000))\n",
        "            # ('clf', RidgeClassifierCV(cv=12))\n",
        "            ])\n",
        "    log_reg = log_reg.fit(data_dict_exp['repr_train_df'], data_dict_exp['price_dir_train'])\n",
        "\n",
        "    # Predict on the training set\n",
        "    predictions_train = log_reg.predict(data_dict_exp['repr_train_df'])\n",
        "\n",
        "    # Predict on the validation set\n",
        "    predictions_val = log_reg.predict(data_dict_exp['repr_val_df'])\n",
        "\n",
        "    # Predict on the test set\n",
        "    predictions_test = log_reg.predict(data_dict_exp['repr_test_df'])\n",
        "\n",
        "\n",
        "    # Calculate the training accuracy\n",
        "    acc_train = accuracy_score(data_dict_exp['price_dir_train'], predictions_train)\n",
        "    print(f\"Training Accuracy: {acc_train}\")\n",
        "\n",
        "    # Calculate the validation accuracy\n",
        "    acc_val = accuracy_score(data_dict_exp['price_dir_val'], predictions_val)\n",
        "    print(f\"Validation Accuracy: {acc_val}\")\n",
        "\n",
        "    # Calculate the test accuracy\n",
        "    acc_test = accuracy_score(data_dict_exp['price_dir_test'], predictions_test)\n",
        "    print(f\"Test Accuracy: {acc_test}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Calculate the training F1 score\n",
        "    f1_train = f1_score(data_dict_exp['price_dir_train'], predictions_train)\n",
        "    print(f\"Training F1 Score: {f1_train}\")\n",
        "\n",
        "    # Calculate the validation F1 score\n",
        "    f1_val = f1_score(data_dict_exp['price_dir_val'], predictions_val)\n",
        "    print(f\"Validation F1 Score: {f1_val}\")\n",
        "\n",
        "    # Calculate the test F1 score\n",
        "    f1_test = f1_score(data_dict_exp['price_dir_test'], predictions_test)\n",
        "    print(f\"Test F1 Score: {f1_test}\")\n",
        "\n",
        "    return {\n",
        "        \"predictions_train\" : predictions_train,\n",
        "        \"predictions_val\" : predictions_val,\n",
        "        \"predictions_test\" : predictions_test,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0: loss=4.54233873807467\n",
            "Epoch #1: loss=3.1405293483000536\n",
            "Epoch #2: loss=2.2901597023010254\n",
            "Epoch #3: loss=1.7088850736618042\n",
            "Epoch #4: loss=1.3129169138578267\n",
            "Epoch #5: loss=1.0713550998614385\n",
            "Epoch #6: loss=0.9414903085965377\n",
            "Epoch #7: loss=0.8452572501622714\n",
            "Epoch #8: loss=0.6293596911888856\n",
            "Epoch #9: loss=0.7607850117179064\n",
            "Epoch #10: loss=0.5253325723684751\n",
            "Epoch #11: loss=0.6330322405466666\n",
            "Epoch #12: loss=0.5192699105693743\n",
            "Epoch #13: loss=0.712838402734353\n",
            "Epoch #14: loss=0.5314407938948045\n",
            "Epoch #15: loss=0.6204881542004071\n",
            "Epoch #16: loss=0.598580430333431\n",
            "Epoch #17: loss=0.4329864875628398\n",
            "Epoch #18: loss=0.4755411400244786\n",
            "Epoch #19: loss=0.45625942658919555\n",
            "Epoch #20: loss=0.4180302064006145\n",
            "Epoch #21: loss=0.4695209144399716\n",
            "Epoch #22: loss=0.4289982244372368\n",
            "Epoch #23: loss=0.4059902085707738\n",
            "Epoch #24: loss=0.40561071038246155\n",
            "Epoch #25: loss=0.3917604065858401\n",
            "Epoch #26: loss=0.4754769905255391\n",
            "Epoch #27: loss=0.3764107513886232\n",
            "Epoch #28: loss=0.3789591032725114\n",
            "Epoch #29: loss=0.3630490583869127\n",
            "Epoch #30: loss=0.361239830748393\n",
            "Epoch #31: loss=0.31139761438736546\n",
            "Epoch #32: loss=0.3392021464040646\n",
            "Epoch #33: loss=0.3569017660159331\n",
            "Epoch #34: loss=0.31208172927682215\n",
            "Epoch #0: loss=4.480651739871863\n",
            "Epoch #1: loss=3.5590372446811562\n",
            "Epoch #2: loss=2.822180227799849\n",
            "Epoch #3: loss=2.156726295297796\n",
            "Epoch #4: loss=1.760203029170181\n",
            "Epoch #5: loss=1.3242790373888882\n",
            "Epoch #6: loss=1.331607076254758\n",
            "Epoch #7: loss=0.9865138313987039\n",
            "Epoch #8: loss=1.1755310369260383\n",
            "Epoch #9: loss=0.990953367767912\n",
            "Epoch #10: loss=1.0441802541414897\n",
            "Epoch #11: loss=0.7813403064554388\n",
            "Epoch #12: loss=0.759770393371582\n",
            "Epoch #13: loss=0.7788622722481237\n",
            "Epoch #14: loss=0.6792893969651425\n",
            "Epoch #15: loss=0.6781893526062821\n",
            "Epoch #16: loss=0.7015605288924593\n",
            "Epoch #17: loss=0.6821520229180654\n",
            "Epoch #18: loss=0.5932856445962732\n",
            "Epoch #19: loss=0.6374279603813634\n",
            "Epoch #20: loss=0.6589900806094661\n",
            "Epoch #21: loss=0.5181933876239893\n",
            "Epoch #22: loss=0.5416049027081692\n",
            "Epoch #23: loss=0.6316387517885729\n",
            "Epoch #24: loss=0.528443225405433\n",
            "Epoch #25: loss=0.5239534025842493\n",
            "Epoch #26: loss=0.5181206383488395\n",
            "Epoch #27: loss=0.5550302790872979\n",
            "Epoch #28: loss=0.5663813524173967\n",
            "Epoch #29: loss=0.48005724133867206\n",
            "Epoch #30: loss=0.46918308554273663\n",
            "Epoch #31: loss=0.43857829679142346\n",
            "Epoch #32: loss=0.4800368068796216\n",
            "Epoch #33: loss=0.45822405544194306\n",
            "Epoch #34: loss=0.45723292592800024\n"
          ]
        }
      ],
      "source": [
        "model, loss_log = get_model_loss(data_dict_array, 'btc', n_epochs=35)\n",
        "model_eth, loss_log_eth = get_model_loss(data_dict_array, 'eth', n_epochs=35)\n",
        "model_avax, loss_log_avax = get_model_loss(data_dict_array, 'avax', n_epochs=35)\n",
        "model_matic, loss_log_matic = get_model_loss(data_dict_array, 'matic', n_epochs=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0: loss=4.231111526489258\n",
            "Epoch #1: loss=2.790238768446679\n",
            "Epoch #2: loss=1.9555433988571167\n",
            "Epoch #3: loss=1.4903870926183813\n",
            "Epoch #4: loss=1.3001563245174932\n",
            "Epoch #5: loss=1.0452503374978608\n",
            "Epoch #6: loss=0.8143194224320206\n",
            "Epoch #7: loss=0.8148404091012245\n",
            "Epoch #8: loss=0.6847831104315963\n",
            "Epoch #9: loss=0.6217954860014074\n",
            "Epoch #10: loss=0.6696575722273659\n",
            "Epoch #11: loss=0.5834437322382834\n",
            "Epoch #12: loss=0.5432782365995295\n",
            "Epoch #13: loss=0.5842249159719429\n",
            "Epoch #14: loss=0.5557943728624606\n",
            "Epoch #15: loss=0.49550967532045703\n",
            "Epoch #16: loss=0.5293030394058601\n",
            "Epoch #17: loss=0.543171939312243\n",
            "Epoch #18: loss=0.4197062925965178\n",
            "Epoch #19: loss=0.42770528092103843\n"
          ]
        }
      ],
      "source": [
        "model_eth, loss_log_eth = get_model_loss(data_dict_array, 'eth', n_epochs=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "btc_data_dict_exp = get_all_data_for_experiment(data_dict_array, temp_df_btc_raw, \"btc\", model)\n",
        "eth_data_dict_exp = get_all_data_for_experiment(data_dict_array, temp_df_eth_raw, \"eth\", model_eth)\n",
        "avax_data_dict_exp = get_all_data_for_experiment(data_dict_array, temp_df_avax_raw, \"avax\", model_avax)\n",
        "matic_data_dict_exp = get_all_data_for_experiment(data_dict_array, temp_df_matic_raw, \"matic\", model_matic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.5218036191041234\n",
            "Validation Accuracy: 0.5037080984870959\n",
            "Test Accuracy: 0.5029655990510083\n",
            "\n",
            "\n",
            "Training F1 Score: 0.5739394740319811\n",
            "Validation F1 Score: 0.5283338032139836\n",
            "Test F1 Score: 0.5601049868766405\n"
          ]
        }
      ],
      "source": [
        "pred_btc = get_train_and_get_f1_acc(btc_data_dict_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_eth = get_train_and_get_f1_acc(eth_data_dict_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.5231533502868295\n",
            "Validation Accuracy: 0.5049692380501657\n",
            "Test Accuracy: 0.5092198581560283\n",
            "\n",
            "\n",
            "Training F1 Score: 0.4688755681443911\n",
            "Validation F1 Score: 0.5004775549188156\n",
            "Test F1 Score: 0.36938031591737547\n"
          ]
        }
      ],
      "source": [
        "pred_avax = get_train_and_get_f1_acc(avax_data_dict_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.5201665935992985\n",
            "Validation Accuracy: 0.49842160645387584\n",
            "Test Accuracy: 0.5014025245441796\n",
            "\n",
            "\n",
            "Training F1 Score: 0.44046827871785693\n",
            "Validation F1 Score: 0.4491525423728814\n",
            "Test F1 Score: 0.3471074380165289\n"
          ]
        }
      ],
      "source": [
        "pred_matic = get_train_and_get_f1_acc(matic_data_dict_exp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # Compute timestamp-level representations for test set\n",
        "# test_repr_tl = model.encode(test_data)  # n_instances x n_timestamps x output_dims\n",
        "\n",
        "# # Compute instance-level representations for test set\n",
        "# test_repr_il = model.encode(test_data, encoding_window='full_series')  # n_instances x output_dims\n",
        "\n",
        "# # Sliding inference for test set\n",
        "# test_repr_si = model.encode(\n",
        "#     test_data,\n",
        "#     casual=True,\n",
        "#     sliding_length=1,\n",
        "#     sliding_padding=50\n",
        "# )  # n_instances x n_timestamps x output_dims\n",
        "# # (The timestamp t's representation vector is computed using the observations located in [t-50, t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lPC5GCUXHnZR"
      },
      "outputs": [],
      "source": [
        "# data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols = datautils.load_forecast_csv(\"ETTh2\")\n",
        "# train_data = data[:, train_slice]\n",
        "        \n",
        "# # Train a TS2Vec model\n",
        "# model = TS2Vec(\n",
        "#     input_dims=1,\n",
        "#     device=0,\n",
        "#     output_dims=320\n",
        "# )\n",
        "# loss_log = model.fit(\n",
        "#     train_data,\n",
        "#     verbose=True\n",
        "# )\n",
        "\n",
        "# # Compute timestamp-level representations for test set\n",
        "# test_repr = model.encode(test_data)  # n_instances x n_timestamps x output_dims\n",
        "\n",
        "# # Compute instance-level representations for test set\n",
        "# test_repr = model.encode(test_data, encoding_window='full_series')  # n_instances x output_dims\n",
        "\n",
        "# # Sliding inference for test set\n",
        "# test_repr = model.encode(\n",
        "#     test_data,\n",
        "#     casual=True,\n",
        "#     sliding_length=1,\n",
        "#     sliding_padding=50\n",
        "# )  # n_instances x n_timestamps x output_dims\n",
        "# # (The timestamp t's representation vector is computed using the observations located in [t-50, t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPYVSTEqHnZR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
